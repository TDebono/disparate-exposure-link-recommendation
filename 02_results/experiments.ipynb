{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "\n",
    "\n",
    "from random_network import *\n",
    "from bba import *\n",
    "from bba_homophily import *\n",
    "from dpah import *\n",
    "import itertools\n",
    "from data_loader import *\n",
    "from fast_pagerank import pagerank_power\n",
    "import random\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from numpy import inf\n",
    "\n",
    "import os\n",
    "import powerlaw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "from collections import Counter\n",
    "from fast_pagerank import pagerank_power\n",
    "\n",
    "import joblib\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGNET = 11000\n",
    "EXT = '.gpickle'\n",
    "TOPK = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ppr(node_index, A, p, top):\n",
    "    pp = np.zeros(A.shape[0])\n",
    "    pp[node_index] = A.shape[0]\n",
    "    pr = pagerank_power(A, p=p, personalize=pp)\n",
    "    pr = pr.argsort()[-top-1:][::-1]\n",
    "    #time.sleep(0.01)\n",
    "    return pr[pr!=node_index][:top]\n",
    "\n",
    "def get_circle_of_trust_per_node(A, p=0.85, top=TOPK, num_cores=40):\n",
    "    return Parallel(n_jobs=num_cores)(delayed(_ppr)(node_index, A, p, top) for node_index in np.arange(A.shape[0]))\n",
    "\n",
    "def frequency_by_circle_of_trust(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=-1):\n",
    "    results = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, -1)\n",
    "    unique_elements, counts_elements = np.unique(np.concatenate(results), return_counts=True)\n",
    "    del(results)\n",
    "    return [ 0 if node_index not in unique_elements else counts_elements[np.argwhere(unique_elements == node_index)[0, 0]] for node_index in np.arange(A.shape[0])]\n",
    "\n",
    "def _salsa(node_index, cot, A, top=TOPK):\n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                       and n not in cot\n",
    "                                                                                       and n != node_index\n",
    "                                                                                       and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "    del(BG)\n",
    "    #time.sleep(0.01)\n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def get_topj_bc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    bc_full = g.betweenness(directed=True)\n",
    "    g.vs['bc'] = bc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        # maj_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_bc_temp = min_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        min_bc_temp.sort()\n",
    "        topj = min_bc_temp[-j:]\n",
    "\n",
    "        inds = [min_bc.index(t) for t in topj][-j:]\n",
    "        \n",
    "    if mode == 'maj':\n",
    "        maj_bc = maj_nodes_ig.get_attribute_values('bc')\n",
    "        maj_bc_temp = maj_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        maj_bc_temp.sort()\n",
    "        topj = maj_bc_temp[-j:]\n",
    "\n",
    "        inds = [maj_bc.index(t) for t in topj][-j:]\n",
    "\n",
    "    return inds\n",
    "\n",
    "def get_topj_evc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    evc_full = g.eigenvector_centrality()\n",
    "    g.vs['evc'] = evc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_evc = min_nodes_ig.get_attribute_values('evc')\n",
    "        # maj_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_evc_temp = min_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        min_evc_temp.sort()\n",
    "        topj = min_evc_temp[-j:]\n",
    "\n",
    "        inds = [min_evc.index(t) for t in topj][-j:]\n",
    "    if mode == 'maj':\n",
    "        maj_evc = maj_nodes_ig.get_attribute_values('evc')\n",
    "        maj_evc_temp = maj_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        maj_evc_temp.sort()\n",
    "        topj = maj_evc_temp[-j:]\n",
    "\n",
    "        inds = [maj_evc.index(t) for t in topj][-j:]\n",
    "\n",
    "    return inds\n",
    "\n",
    "def DEPR_salsa_intervention(node_index, cot, A, top=TOPK, intervene=False, to_add=None, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "        \n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                       and n not in cot\n",
    "                                                                                       and n != node_index\n",
    "                                                                                       and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "    del(BG)\n",
    "    \n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def _salsa2(node_index, cot, A, top=TOPK):\n",
    "    \n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                       and n not in cot\n",
    "                                                                                       and n != node_index\n",
    "                                                                                       and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "\n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def _get_hubs_auths(node_index, cot, A, top=TOPK):\n",
    "    \n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "\n",
    "    auths = [node for node, data in BG.nodes(data=True) if data['bipartite'] == 1]\n",
    "    hubs = [int(node[1:]) for node, data in BG.nodes(data=True) if data['bipartite'] == 0]\n",
    "\n",
    "    return {node_index: {'hubs': hubs, 'auths': auths}}\n",
    "\n",
    "def frequency_by_who_to_follow(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=40):\n",
    "    cot_per_node = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, -1)\n",
    "    results = Parallel(n_jobs=-1)(delayed(_salsa)(node_index, cot, A, top) for node_index, cot in enumerate(cot_per_node))\n",
    "    unique_elements, counts_elements = np.unique(np.concatenate(results), return_counts=True)\n",
    "    del(results)\n",
    "    return [0 if node_index not in unique_elements else counts_elements[np.argwhere(unique_elements == node_index)[0, 0]] for node_index in np.arange(A.shape[0])]\n",
    "\n",
    "def frequency_by_who_to_follow2(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=-1):\n",
    "\n",
    "    cot_per_node = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, num_cores)\n",
    "    results = Parallel(n_jobs=-1)(delayed(_salsa2)(node_index, cot, A, top) for node_index, cot in enumerate(cot_per_node))\n",
    "\n",
    "    return results\n",
    "\n",
    "def frequency_by_who_to_follow3(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=-1):\n",
    "\n",
    "    cot_per_node = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, num_cores)\n",
    "    ha_dict = Parallel(n_jobs=-1)(delayed(_get_hubs_auths)(node_index, cot, A, top) for node_index, cot in enumerate(cot_per_node))\n",
    "\n",
    "    return ha_dict\n",
    "\n",
    "def who_to_follow_rank(A, njobs=-1):\n",
    "    if A.shape[0] < BIGNET:\n",
    "        return wtf_small(A=A, njobs=njobs)\n",
    "    else:\n",
    "        return wtf_small(A=A, njobs=njobs)\n",
    "        \n",
    "def wtf_small(A, njobs):\n",
    "    # utils.printf('cot_per_node...')\n",
    "    cot_per_node = get_circle_of_trust_per_node(A, p=0.85, top=TOPK, num_cores=njobs)\n",
    "\n",
    "    # utils.printf('cot...')\n",
    "    # cot = frequency_by_circle_of_trust(A, cot_per_node=cot_per_node, p=0.85, top=TOPK, num_cores=njobs)\n",
    "\n",
    "    # utils.printf('wtf...')\n",
    "    wtf = frequency_by_who_to_follow3(A=A, cot_per_node=cot_per_node, p=0.85, top=TOPK, num_cores=njobs)\n",
    "    return wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_wtf(network, sparse_adj, node_id, k_for_circle_of_trust=20, cot_factor= 0.05, tol=1e-8,\n",
    "                damping_factor=.85, k_for_recommendation=-1):\n",
    "    \"\"\"This method aims to realize a link prediction algorithm used by Twitter to perform\n",
    "        the WTF recommendation on the platform.\n",
    "        The algorithm can be seen at 'https://web.stanford.edu/~rezab/papers/wtf_overview.pdf'.\n",
    "        The algorithm consists of two phases:\n",
    "            1) Compute the circle of trust for the user you want to recommend(top-k nodes in PPR)\n",
    "            2) Compute the top-k nodes using score propagation\n",
    "    \"\"\"\n",
    "    k_for_circle_of_trust = min(int(network.number_of_nodes()*cot_factor), k_for_circle_of_trust)\n",
    "    #1st phase: Compute circle of trust of user according to Personalized PageRank\n",
    "    personalize = np.zeros(shape=network.number_of_nodes())\n",
    "    personalize[node_id] = 1\n",
    "    values_of_personalized_pr = pagerank_power(sparse_adj, p=damping_factor, personalize=personalize, tol=1e-6)\n",
    "    circle_of_trust = values_of_personalized_pr.argsort()[-k_for_circle_of_trust:][::-1]\n",
    "\n",
    "    #2nd phase: init bipartite graph\n",
    "    bipartite_graph = nx.DiGraph()\n",
    "    #add nodes belonging to the circle of trust as hubs(H)\n",
    "    for node in circle_of_trust:\n",
    "        #these nodes are \"hubs\"(H) in the bipartite graph\n",
    "        bipartite_graph.add_node(str(node)+\"H\")\n",
    "    #add out neighbors of nodes belonging to the circle of trust as authorities(A)\n",
    "    for node in circle_of_trust:\n",
    "        for out_neighbor in network.neighbors(node):\n",
    "            #direction is inverted for a matter of simplicity in the sequent phases\n",
    "            bipartite_graph.add_edge(str(out_neighbor)+\"A\", str(node)+\"H\")\n",
    "\n",
    "    #retrieve adjacency matrix of bipartite graph\n",
    "    A = nx.to_numpy_array(bipartite_graph)\n",
    "\n",
    "    #retrieve list of all nodes splitted by authority or hub\n",
    "    all_nodes = list(bipartite_graph.nodes())\n",
    "    hub_nodes = [int(x[:-1]) for x in all_nodes if 'H' in x]\n",
    "    authority_nodes = [int(x[:-1]) for x in all_nodes if 'A' in x]\n",
    "\n",
    "    #3rd phase: start building ingredients of our SALSA algorithm\n",
    "    #these are the transition matrices determined by the bipartite graph\n",
    "    S_prime = A[len(hub_nodes):, :][:, :len(hub_nodes)].copy()\n",
    "    R_prime = S_prime.T.copy()\n",
    "    #normalize both matrices\n",
    "    denominator_S_prime = S_prime.sum(axis=0)\n",
    "    denominator_S_prime[denominator_S_prime == 0] = 1\n",
    "    S_prime = S_prime / denominator_S_prime\n",
    "    denominator_R_prime = R_prime.sum(axis=0)\n",
    "    denominator_R_prime[denominator_R_prime == 0] = 1\n",
    "    R_prime = R_prime / denominator_R_prime\n",
    "    #these are the vectors which contain the score of similarity\n",
    "    #and relevance\n",
    "    s = np.zeros(shape=(len(hub_nodes), 1), dtype=np.float)\n",
    "    r = np.zeros(shape=(len(authority_nodes), 1), dtype=np.float)\n",
    "\n",
    "    #at the beginning of the procedure we put the similarity\n",
    "    #of the user we want to give the recommendation equal to 1\n",
    "    index_of_node_to_recommend = np.where(circle_of_trust == node_id)[0][0]\n",
    "    s[index_of_node_to_recommend] = 1.\n",
    "\n",
    "    #init damping vector\n",
    "    alpha = 1 - damping_factor\n",
    "    alpha_vector = np.zeros(shape=(len(hub_nodes), 1), dtype=np.float)\n",
    "    alpha_vector[index_of_node_to_recommend] = alpha\n",
    "\n",
    "    #4th phase: run the algorithm\n",
    "    convergence = False\n",
    "    while not convergence:\n",
    "        s_ = s.copy()\n",
    "        r_ = r.copy()\n",
    "        r_ = S_prime.dot(s)\n",
    "        s_ = alpha_vector + (1 - alpha)*(R_prime.dot(r))\n",
    "        #compute difference and check if convergence has been reached\n",
    "        diff = abs(s_ - s)\n",
    "        if np.linalg.norm(diff) < tol:\n",
    "            convergence=True\n",
    "        #update real vectors\n",
    "        s = s_\n",
    "        r = r_\n",
    "\n",
    "    #5th phase: order by score and delete neighbors of node to be recommended\n",
    "    #of course we don't want to recommend people that the user already follow\n",
    "    neighbors_to_not_recommend = nx.neighbors(network, node_id)\n",
    "    relevance_scores = r.flatten()\n",
    "    if k_for_recommendation == -1:\n",
    "        k_for_recommendation = 0 #Take all the nodes!\n",
    "\n",
    "    neighbors_to_not_recommend = set(neighbors_to_not_recommend)\n",
    "    results = []\n",
    "    for node in relevance_scores.argsort()[::-1]:\n",
    "        if node not in neighbors_to_not_recommend and node != node_id:\n",
    "            results.append(((node_id, node))) # , relevance_scores[node]\n",
    "            if len(results) == k_for_recommendation:\n",
    "                break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topj_bc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    bc_full = g.betweenness(directed=True)\n",
    "    g.vs['bc'] = bc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        # maj_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_bc_temp = min_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        min_bc_temp.sort()\n",
    "        topj = min_bc_temp[-j:]\n",
    "\n",
    "        inds = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "\n",
    "    if mode == 'maj':\n",
    "        maj_bc = maj_nodes_ig.get_attribute_values('bc')\n",
    "        maj_bc_temp = maj_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        maj_bc_temp.sort()\n",
    "        topj = maj_bc_temp[-j:]\n",
    "\n",
    "        inds = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "    \n",
    "    elif mode == 'both':\n",
    "        min_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_bc_temp = min_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        min_bc_temp.sort()\n",
    "        topj = min_bc_temp[-j:]\n",
    "\n",
    "        inds_min = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        maj_bc = maj_nodes_ig.get_attribute_values('bc')\n",
    "        maj_bc_temp = maj_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        maj_bc_temp.sort()\n",
    "        topj = maj_bc_temp[-j:]\n",
    "\n",
    "        inds_maj = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds_maj, inds_min\n",
    "\n",
    "\n",
    "def get_topj_evc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    evc_full = g.eigenvector_centrality()\n",
    "    g.vs['evc'] = evc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_evc = min_nodes_ig.get_attribute_values('evc')\n",
    "        min_evc_temp = min_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        min_evc_temp.sort()\n",
    "        topj = min_evc_temp[-j:]\n",
    "\n",
    "        inds = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "    \n",
    "    elif mode == 'maj':\n",
    "        maj_evc = maj_nodes_ig.get_attribute_values('evc')\n",
    "        maj_evc_temp = maj_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        maj_evc_temp.sort()\n",
    "        topj = maj_evc_temp[-j:]\n",
    "\n",
    "        inds = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "\n",
    "    elif mode == 'both':\n",
    "        min_evc = min_nodes_ig.get_attribute_values('evc')\n",
    "        min_evc_temp = min_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        min_evc_temp.sort()\n",
    "        topj = min_evc_temp[-j:]\n",
    "\n",
    "        inds_min = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        maj_evc = maj_nodes_ig.get_attribute_values('evc')\n",
    "        maj_evc_temp = maj_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        maj_evc_temp.sort()\n",
    "        topj = maj_evc_temp[-j:]\n",
    "\n",
    "        inds_maj = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds_maj, inds_min\n",
    "\n",
    "\n",
    "def _ppr(node_index, A, p, top):\n",
    "    pp = np.zeros(A.shape[0])\n",
    "    pp[node_index] = A.shape[0]\n",
    "    pr = pagerank_power(A, p=p, personalize=pp)\n",
    "    pr = pr.argsort()[-top-1:][::-1]\n",
    "    #time.sleep(0.01)\n",
    "    return list(pr[pr!=node_index][:top])\n",
    "\n",
    "def _get_circle_of_trust_per_node(A, p=0.85, top=10, num_cores=-1):\n",
    "    return Parallel(n_jobs=num_cores)(delayed(_ppr)(node_index, A, p, top) for node_index in np.arange(A.shape[0]))\n",
    "\n",
    "def _salsa_intervention(node_index, cot, A, top=10, to_add=[], network_obj=None):\n",
    "\n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "\n",
    "    auths = [node for node, data in BG.nodes(data=True) if data['bipartite'] == 1]\n",
    "    hubs = [int(node[1:]) for node, data in BG.nodes(data=True) if data['bipartite'] == 0]\n",
    "\n",
    "    if network_obj.nodes()[node_index]['m'] == 1:\n",
    "        to_add = to_add[1]\n",
    "    else:\n",
    "        to_add = to_add[0]\n",
    "\n",
    "        new_auths = [auth_node for auth_node in to_add if auth_node not in auths]\n",
    "        new_auths_not_hubs = [n for n in new_auths if n not in hubs]\n",
    "\n",
    "        intervention_edges = [(hub, new_auth) for hub in random.sample(hubs, int(len(hubs) * 0.15)) for new_auth in new_auths_not_hubs]\n",
    "        BG.add_nodes_from(new_auths_not_hubs, bipartite=1)\n",
    "        BG.add_edges_from(intervention_edges)\n",
    "\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                        and n not in cot\n",
    "                                                                                        and n != node_index\n",
    "                                                                                        and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "\n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def get_intervention_recommendations(A, to_add=[], p=0.85, top=10, num_cores=-1, network_obj=None):\n",
    "    \n",
    "    cot_per_node = _get_circle_of_trust_per_node(A, p, top, num_cores)\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(_salsa_intervention)(node_index, cot, A, top, to_add, network_obj) for node_index, cot in enumerate(cot_per_node))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "\n",
    "    def __init__(self, network, class_name='m', min_label=1, maj_label=0):\n",
    "        self.network = network\n",
    "        self.min_label, self.maj_label = min_label, maj_label\n",
    "        # self.min_nodes, self.maj_nodes = self.get_nodes_per_class()\n",
    "        # self.post_network = None\n",
    "        self.class_name = class_name\n",
    "        self.metrics_dict = {}\n",
    "    \n",
    "    def get_nodes_per_class(self, network):\n",
    "        \n",
    "        # class_\n",
    "        if self.class_name == 'm':\n",
    "            min_nodes_ig = network.vs.select(m=self.min_label)\n",
    "            maj_nodes_ig = network.vs.select(m=self.maj_label)\n",
    "\n",
    "        elif self.class_name == 'class_':\n",
    "            min_nodes_ig = network.vs.select(class_=self.min_label)\n",
    "            maj_nodes_ig = network.vs.select(class_=self.maj_label)\n",
    "        \n",
    "        elif self.class_name == 'age':\n",
    "            min_nodes_ig = network.vs.select(age=self.min_label)\n",
    "            maj_nodes_ig = network.vs.select(age=self.maj_label)\n",
    "\n",
    "        return min_nodes_ig, maj_nodes_ig\n",
    "    \n",
    "    def get_group_betweenness(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        bc_full = network.betweenness(directed=True)\n",
    "        network.vs['bc'] = bc_full\n",
    "\n",
    "        min_bc = np.average(np.array(min_nodes.get_attribute_values('bc')))\n",
    "        maj_bc = np.average(np.array(maj_nodes.get_attribute_values('bc')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['bc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['bc']['pre'] = {'min': min_bc, 'maj': maj_bc}\n",
    "        else:\n",
    "            self.metrics_dict['bc']['post'] = {'min': min_bc, 'maj': maj_bc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority BC: {min_bc}\")\n",
    "            print(f\"Majority BC: {maj_bc}\")\n",
    "\n",
    "        return min_bc, maj_bc\n",
    "    \n",
    "    def get_group_closeness(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        cc_full = network.closeness()\n",
    "        network.vs['cc'] = cc_full\n",
    "\n",
    "        min_cc = np.average(np.array(min_nodes.get_attribute_values('cc')))\n",
    "        maj_cc = np.average(np.array(maj_nodes.get_attribute_values('cc')))\n",
    "\n",
    "        if is_pre:\n",
    "            self.metrics_dict['cc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['cc']['pre'] = {'min': min_cc, 'maj': maj_cc}\n",
    "        else:\n",
    "            self.metrics_dict['cc']['post'] = {'min': min_cc, 'maj': maj_cc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority CC: {min_cc}\")\n",
    "            print(f\"Majority CC: {maj_cc}\")\n",
    "\n",
    "        return min_cc, maj_cc\n",
    "    \n",
    "    def get_group_eigenvector_centrality(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        evc_full = network.eigenvector_centrality()\n",
    "        network.vs['evc'] = evc_full\n",
    "\n",
    "        min_evc = np.average(np.array(min_nodes.get_attribute_values('evc')))\n",
    "        maj_evc = np.average(np.array(maj_nodes.get_attribute_values('evc')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['evc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['evc']['pre'] = {'min': min_evc, 'maj': maj_evc}\n",
    "        else:\n",
    "            self.metrics_dict['evc']['post'] = {'min': min_evc, 'maj': maj_evc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority EVC: {min_evc} / normalized: {min_evc/(min_evc+maj_evc)}\")\n",
    "            print(f\"Majority EVC: {maj_evc} / normalized: {maj_evc/(min_evc+maj_evc)}\")\n",
    "\n",
    "        return min_evc, maj_evc\n",
    "    \n",
    "    def get_group_degree_centrality(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        dc_full = network.vs.indegree()\n",
    "        network.vs['dc'] = dc_full\n",
    "\n",
    "        min_dc = np.average(np.array(min_nodes.get_attribute_values('dc')))\n",
    "        maj_dc = np.average(np.array(maj_nodes.get_attribute_values('dc')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['dc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['dc']['pre'] = {'min': min_dc, 'maj': maj_dc}\n",
    "        else:\n",
    "            self.metrics_dict['dc']['post'] = {'min': min_dc, 'maj': maj_dc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority DC: {min_dc}\")\n",
    "            print(f\"Majority DC: {maj_dc}\")\n",
    "\n",
    "        return min_dc, maj_dc\n",
    "\n",
    "    def get_group_pagerank(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "        pr_full = network.pagerank()\n",
    "\n",
    "        network.vs['pr'] = pr_full\n",
    "\n",
    "        min_pr = np.average(np.array(min_nodes.get_attribute_values('pr')))\n",
    "        maj_pr = np.average(np.array(maj_nodes.get_attribute_values('pr')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['pr'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['pr']['pre'] = {'min': min_pr, 'maj': maj_pr}\n",
    "        else:\n",
    "            self.metrics_dict['pr']['post'] = {'min': min_pr, 'maj': maj_pr}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority PR: {min_pr}\")\n",
    "            print(f\"Majority PR: {maj_pr}\")\n",
    "\n",
    "        return min_pr, maj_pr\n",
    "    \n",
    "    def remove_zero_degree(self, verbose=False):\n",
    "        self.to_remove = [node for node, degree in dict(self.network.degree()).items() if degree < 1]\n",
    "        self.network.remove_nodes_from(self.to_remove)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Removed {len(self.to_remove)} nodes with degree zero.\")\n",
    "        return self.to_remove\n",
    "\n",
    "    def add_nodes(self):\n",
    "        self.network.add_nodes_from(self.to_remove)\n",
    "        # print(f\"Added {len(self.to_remove)} nodes.\")\n",
    "\n",
    "    def sample_and_delete_edges(self, num_to_sample=100, remove_edges=True):\n",
    "        edge_list = list(self.network.edges())\n",
    "\n",
    "        self.random_edges_to_delete = random.sample(edge_list, num_to_sample)\n",
    "        nodes_with_del_out_edge = [tup[0] for tup in self.random_edges_to_delete]\n",
    "        self.unique_nodes = list(set(nodes_with_del_out_edge))\n",
    "\n",
    "        self.nodes_with_edge_count = Counter(node for node in nodes_with_del_out_edge)\n",
    "\n",
    "        if remove_edges:\n",
    "            self.network.remove_edges_from(self.random_edges_to_delete)\n",
    "\n",
    "            # print(f'# nodes with deleted out edge {len(nodes_with_del_out_edge)}')\n",
    "            print(f'# unique nodes with deleted out edge: {len(self.unique_nodes)}')\n",
    "        else:\n",
    "            print(f'# unique nodes with sampled out edge: {len(self.unique_nodes)} - No edges where deleted.')\n",
    "\n",
    "        return self.random_edges_to_delete, self.unique_nodes, self.nodes_with_edge_count\n",
    "\n",
    "    def reconstruct_edges(self, strategy='bulk', k_for_circle_of_trust=20, salsa_new=True, run=0, wtf=None, to_pick=1,\n",
    "                            consider_all_nodes=True, num_to_consider=1000, add_top_k=False, delete_edges=True, \n",
    "                            delete_first=False, network_file='', network_obj=None, class_name='m',\n",
    "                            intervene=False, to_add=None, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "\n",
    "\n",
    "        if strategy == 'intervention':\n",
    "\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            self.top_k_frac_min = []\n",
    "            self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "\n",
    "            all_nodes = list(self.network.nodes())\n",
    "            sample = random.sample(all_nodes, num_to_consider)\n",
    "            evc = nx.eigenvector_centrality(self.network)\n",
    "\n",
    "            for node in sample:\n",
    "\n",
    "                wtf_node_i = list(wtf[node])\n",
    "\n",
    "                if len(wtf_node_i) >= to_pick:\n",
    "                    self.salsa_edges_per_node[node] = []\n",
    "\n",
    "                    sub_dict = {i: evc[i] for i in wtf_node_i}\n",
    "                    sub_dict_sorted = dict(sorted(sub_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "                    pick = list(sub_dict_sorted.keys())[:to_pick]\n",
    "\n",
    "\n",
    "                    if delete_edges:\n",
    "\n",
    "                        out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                        # num_to_remove = min(len(out_edges_node), to_pick)\n",
    "\n",
    "                        # self.network.remove_edges_from(random.sample(out_edges_node, num_to_remove))\n",
    "\n",
    "                        if len(out_edges_node) >= to_pick:\n",
    "                            self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                            for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                            self.salsa_edges_per_node[node].append(pick)   \n",
    "\n",
    "                            top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                            frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                            frac_maj = 1-frac_min\n",
    "                            self.top_k_frac_min.append(frac_min*100)                                \n",
    "\n",
    "\n",
    "        if strategy == 'new':\n",
    "\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            self.top_k_frac_min = []\n",
    "            self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "\n",
    "\n",
    "            top = 10\n",
    "            p = 0.85\n",
    "\n",
    "            sample = random.sample(self.network.nodes(), num_to_consider)\n",
    "\n",
    "\n",
    "            for k in range(to_pick):\n",
    "                \n",
    "                A = nx.adjacency_matrix(self.network)\n",
    "\n",
    "                cots = Parallel(n_jobs=-1)(delayed(_ppr)(node, A, p, top) for node in sample)\n",
    "                z = dict(zip(sample, cots))\n",
    "                recommendations = Parallel(n_jobs=-1)(delayed(_salsa2)(node, z[node], A, ix=None, top=10) for node in z.keys())\n",
    "\n",
    "                for arr in recommendations:\n",
    "\n",
    "                    if network_file == '':\n",
    "                            network = network_obj\n",
    "                    else:\n",
    "                            network = read_pkl_graph(network_file)\n",
    "                    \n",
    "                    top_k_classes = [network.nodes()[node][class_name] for node in arr]\n",
    "                    frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                    frac_maj = 1-frac_min\n",
    "                    self.top_k_frac_min.append(frac_min*100)\n",
    "\n",
    "                zz = dict(zip(sample, recommendations))\n",
    "                for key in zz.keys():\n",
    "                    to_add = zz[key][0]\n",
    "                    self.network.add_edge(key, to_add)\n",
    "                    self.salsa_all_edges.append((key, to_add))\n",
    "                    self.network.remove_edges_from(random.sample(list(self.network.out_edges(key)), 1))    \n",
    "\n",
    "\n",
    "        if strategy == 'consecutively':\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            self.top_k_frac_min = []\n",
    "            self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "            num_edges_added = 0\n",
    "\n",
    "            if salsa_new:\n",
    "\n",
    "                wtf = wtf\n",
    "\n",
    "                if consider_all_nodes:\n",
    "                \n",
    "                    for node in self.network.nodes():\n",
    "                        # num_to_add = self.nodes_with_edge_count[node]\n",
    "\n",
    "                        wtf_node_i = list(wtf[node])\n",
    "                        if len(wtf_node_i) >= to_pick:\n",
    "                            self.salsa_edges_per_node[node] = []\n",
    "\n",
    "                            if add_top_k:\n",
    "                                pick = wtf_node_i[0:to_pick]\n",
    "                                \n",
    "                            else:\n",
    "                                pick = random.sample(wtf_node_i, to_pick)\n",
    "\n",
    "                            \n",
    "                            if delete_edges:\n",
    "                                out_edges_node = list(self.network.out_edges(node))\n",
    "                               \n",
    "                                if len(out_edges_node) >= to_pick:\n",
    "                                    self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                                    for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                    self.salsa_edges_per_node[node].append(pick)\n",
    "\n",
    "                                    top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                    frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                    frac_maj = 1-frac_min\n",
    "                                    self.top_k_frac_min.append(frac_min*100)                                \n",
    "                                    \n",
    "                            else:\n",
    "                                for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                self.salsa_edges_per_node[node].append(pick)\n",
    "\n",
    "                                top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                frac_maj = 1-frac_min\n",
    "                                self.top_k_frac_min.append(frac_min*100)                                \n",
    "                            \n",
    "                                num_edges_added += 1\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    all_nodes = list(self.network.nodes())\n",
    "                    sample = random.sample(all_nodes, num_to_consider)\n",
    "\n",
    "                    for node in sample:\n",
    "\n",
    "                        wtf_node_i = list(wtf[node])\n",
    "\n",
    "\n",
    "                        if len(wtf_node_i) >= to_pick:\n",
    "                            self.salsa_edges_per_node[node] = []\n",
    "\n",
    "                            if add_top_k:\n",
    "                                pick = wtf_node_i[0:to_pick]\n",
    "                            else:\n",
    "                                pick = random.sample(wtf_node_i, to_pick)\n",
    "\n",
    "                            if delete_edges:\n",
    "\n",
    "                                out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                                # num_to_remove = min(len(out_edges_node), to_pick)\n",
    "\n",
    "                                # self.network.remove_edges_from(random.sample(out_edges_node, num_to_remove))\n",
    "\n",
    "                                # for new_node in pick:\n",
    "                                #     self.network.add_edge(node, new_node)\n",
    "                                #     self.salsa_all_edges.append((node, new_node))\n",
    "                                # self.salsa_edges_per_node[node].append(pick)   \n",
    "\n",
    "                                # top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                # frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                # frac_maj = 1-frac_min\n",
    "                                # self.top_k_frac_min.append(frac_min*100)    \n",
    "\n",
    "\n",
    "                                if len(out_edges_node) >= to_pick:\n",
    "                                    self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                                    for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                    self.salsa_edges_per_node[node].append(pick)   \n",
    "\n",
    "                                    top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                    frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                    frac_maj = 1-frac_min\n",
    "                                    self.top_k_frac_min.append(frac_min*100)                                \n",
    "\n",
    "                            else:\n",
    "\n",
    "                                for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                self.salsa_edges_per_node[node].append(pick)   \n",
    "\n",
    "                                top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                frac_maj = 1-frac_min\n",
    "                                self.top_k_frac_min.append(frac_min*100) \n",
    "                                \n",
    "            \n",
    "            else: \n",
    "\n",
    "                for node in self.unique_nodes:\n",
    "                    self.salsa_edges_per_node[node] = []\n",
    "                    for k in range(self.nodes_with_edge_count[node]):\n",
    "\n",
    "                        top_all = twitter_wtf(network=self.network, sparse_adj=nx.adjacency_matrix(self.network), node_id=node, k_for_recommendation=-1)\n",
    "                        top_1 = top_all[0]\n",
    "                        self.network.add_edge(node, top_1[1])\n",
    "                        \n",
    "                        top_10pct = top_all[:int(len(top_all)*0.1)]\n",
    "                        \n",
    "                        # top_10pct_classes = [self.network.nodes()[edge[1]][self.class_name] for edge in top_10pct]\n",
    "                        top_10pct_classes = [self.network.nodes()[edge[-1]][self.class_name] for edge in top_10pct]\n",
    "                        frac_min = sum(top_10pct_classes) / len(top_10pct_classes)\n",
    "                        frac_maj = 1-frac_min\n",
    "                        self.top_10pct_frac_min.append(frac_min)\n",
    "\n",
    "                        self.salsa_edges_per_node[node].append(top_1)\n",
    "                        self.salsa_all_edges.append(top_1)\n",
    "                        \n",
    "                        num_edges_added += 1   \n",
    "        \n",
    "        elif strategy == 'random':\n",
    "\n",
    "            if consider_all_nodes:\n",
    "\n",
    "                self.salsa_edges_per_node = {}\n",
    "                self.salsa_all_edges = []\n",
    "                self.top_k_frac_min = []\n",
    "                self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "                \n",
    "                self.all_nodes = list(self.network.nodes())\n",
    "                for node in self.all_nodes:\n",
    "                    \n",
    "                    pick = random.sample(self.all_nodes, to_pick)\n",
    "\n",
    "                    if delete_edges:\n",
    "                        out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                        if len(out_edges_node) >= to_pick:\n",
    "                            self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                            for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                            self.salsa_edges_per_node[node] = pick\n",
    "                    else:\n",
    "                        for new_node in pick:\n",
    "                            self.network.add_edge(node, new_node)\n",
    "                            self.salsa_all_edges.append((node, new_node))\n",
    "                        self.salsa_edges_per_node[node] = pick\n",
    "            \n",
    "            else:\n",
    "                self.salsa_edges_per_node = {}\n",
    "                self.salsa_all_edges = []\n",
    "                self.top_k_frac_min = []\n",
    "                self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "                \n",
    "                self.all_nodes = list(self.network.nodes())\n",
    "                k = random.sample(self.all_nodes, num_to_consider)\n",
    "\n",
    "                for node in k:\n",
    "                    \n",
    "                    pick = random.sample(k, to_pick)\n",
    "\n",
    "                    if delete_edges:\n",
    "                        out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                        if len(out_edges_node) >= to_pick:\n",
    "                            self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                            for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                            self.salsa_edges_per_node[node] = pick\n",
    "\n",
    "                    else:\n",
    "                        for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                        self.salsa_edges_per_node[node] = pick\n",
    "\n",
    "        self.avg_frac_min = 0\n",
    "\n",
    "        if strategy != 'random':\n",
    "            self.avg_frac_min, self.sd_frac_min = np.round(np.average(self.top_k_frac_min), 5), np.round(np.std(self.top_k_frac_min), 5)\n",
    "\n",
    "\n",
    "        return self.salsa_edges_per_node, self.salsa_all_edges, self.edge_type, self.avg_frac_min\n",
    "\n",
    "    def add_edges(self, strategy='bulk', k_for_circle_of_trust=20):\n",
    "\n",
    "        if strategy == 'bulk':\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            num_edges_added = 0\n",
    "            for node in self.unique_nodes:\n",
    "\n",
    "                recommended_nodes = twitter_wtf(network=self.network, sparse_adj=nx.adjacency_matrix(self.network), node_id=node, k_for_circle_of_trust=k_for_circle_of_trust, k_for_recommendation=self.nodes_with_edge_count[node])\n",
    "                self.network.add_edges_from(recommended_nodes)\n",
    "                self.salsa_edges_per_node[node] = recommended_nodes\n",
    "                self.salsa_all_edges.append(recommended_nodes[0])\n",
    "                num_edges_added += len(recommended_nodes)\n",
    "\n",
    "        elif strategy == 'random':\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            num_edges_added = 0\n",
    "            self.all_nodes = list(self.network.nodes())\n",
    "            for node in self.unique_nodes:\n",
    "                \n",
    "                for z in range(self.nodes_with_edge_count[node]):\n",
    "                    pick = random.sample(self.all_nodes, 1)[0]\n",
    "\n",
    "                    self.network.add_edge(node, pick)\n",
    "                    self.salsa_edges_per_node[node] = pick\n",
    "                    self.salsa_all_edges.append(pick)\n",
    "                    num_edges_added += 1\n",
    "        \n",
    "        return self.salsa_edges_per_node, self.salsa_all_edges\n",
    "\n",
    "    def get_relative_metric_changes(self, metric='all', round=5):\n",
    "\n",
    "        if metric == 'all':\n",
    "           \n",
    "            bc_vals = self.metrics_dict['bc']\n",
    "            print(\"Relative BC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((bc_vals['post']['maj']/bc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((bc_vals['post']['min']/bc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "        \n",
    "            cc_vals = self.metrics_dict['cc']\n",
    "            print(\"Relative CC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((cc_vals['post']['maj']/cc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((cc_vals['post']['min']/cc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "        \n",
    "            evc_vals = self.metrics_dict['evc']\n",
    "            print(\"Relative EVC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((evc_vals['post']['maj']/evc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            # print(f\"Normalized Majority change (pre->post) %: {np.round(((evc_vals['post']['maj']/(evc_vals['post']['maj']+evc_vals['post']['min']))/(evc_vals['pre']['maj']/(evc_vals['pre']['maj']+evc_vals['pre']['min']))-1)*100)}%\")\n",
    "            # print(f\"Normalized Minority change (pre->post) %: {np.round((evc_vals['post']['min']/(evc_vals['pre']['min']+evc_vals['pre']['maj'])-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "\n",
    "            dc_vals = self.metrics_dict['dc']\n",
    "            print(\"Relative DC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((dc_vals['post']['maj']/dc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((dc_vals['post']['min']/dc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "\n",
    "        elif metric == 'bc':\n",
    "            bc_vals = self.metrics_dict['bc']\n",
    "\n",
    "            print(\"Relative BC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((bc_vals['post']['maj']/bc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((bc_vals['post']['min']/bc_vals['pre']['min']-1)*100, round)}%\")\n",
    "        \n",
    "        elif metric == 'cc':\n",
    "            cc_vals = self.metrics_dict['cc']\n",
    "            \n",
    "            print(\"Relative CC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((cc_vals['post']['maj']/cc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((cc_vals['post']['min']/cc_vals['pre']['min']-1)*100, round)}%\")\n",
    "        \n",
    "        elif metric == 'evc':\n",
    "            evc_vals = self.metrics_dict['evc']\n",
    "            \n",
    "            print(\"Relative EVC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((evc_vals['post']['maj']/evc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round)}%\")\n",
    "        \n",
    "        elif metric == 'dc':\n",
    "            dc_vals = self.metrics_dict['dc']\n",
    "            \n",
    "            print(\"Relative DC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((dc_vals['post']['maj']/dc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((dc_vals['post']['min']/dc_vals['pre']['min']-1)*100, round)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_to_ig(G):\n",
    "    return ig.Graph.from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_class_attribute(G, from_='', to_=''):\n",
    "    class_ = G.vs()[from_]\n",
    "    G.vs[to_] = class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_degree(network, verbose=False):\n",
    "    to_remove = [node for node, degree in dict(network.degree()).items() if degree < 1]\n",
    "    network.remove_nodes_from(to_remove)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Removed {len(to_remove)} nodes with degree zero.\")\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_simulations(network_obj=None, network_file='', n=5, num_to_sample=10000, delete_edges=True, verbose=False, round=5, \n",
    "                      remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                      strategy='bulk', save_results=True, save_as='', to_txt=True, salsa_new=True, quick_test=False,\n",
    "                      to_pick=1, consider_all_nodes=True, num_to_consider=1000, add_top_k=False,\n",
    "                      intervene=False, strat='evc', metric_mode='both', j=5, target_mode='min', recomms=None):\n",
    "\n",
    "    \n",
    "\n",
    "    changes_dict_min = {'bc': [],\n",
    "                        'cc': [],\n",
    "                        'evc_norm': [],\n",
    "                        'evc': [],\n",
    "                        'evc_abs': [],\n",
    "                        'dc': [],\n",
    "                        'pr': [],\n",
    "                        'c_coeff': [],\n",
    "                        'aspl': []\n",
    "                        }\n",
    "\n",
    "    changes_dict_maj = {'bc': [],\n",
    "                        'cc': [],\n",
    "                        'evc_norm': [],\n",
    "                        'evc': [],\n",
    "                        'evc_abs': [],\n",
    "                        'dc': [],\n",
    "                        'pr': [],\n",
    "                        'c_coeff': [],\n",
    "                        'aspl': []\n",
    "                        }\n",
    "\n",
    "    recommended_nodes_dict = {}\n",
    "\n",
    "    edge_type_dict = {}\n",
    "\n",
    "    frac_mins_list = []\n",
    "\n",
    "    # overlaps = []\n",
    "    \n",
    "    # wtf = None\n",
    "\n",
    "    # if salsa_new and not intervene:\n",
    "    wtf = who_to_follow_rank(A=nx.adjacency_matrix(network_obj), njobs=-1)\n",
    "    \n",
    "    # elif intervene:\n",
    "    # wtf = recomms\n",
    "        \n",
    "    \n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        if network_file == '':\n",
    "            g = network_obj\n",
    "        else:\n",
    "            g = read_pkl_graph(network_file)\n",
    "       \n",
    "        g = reindex_nodes(g, name_as_str=False)\n",
    "\n",
    "        a = Analyzer(network=g, class_name=class_name, min_label=min_label, maj_label=maj_label)\n",
    "\n",
    "        if remove_zero_degree_nodes:\n",
    "            _ = a.remove_zero_degree()\n",
    "\n",
    "        if not quick_test:\n",
    "            if i == 0:\n",
    "                min_c_coeff, maj_c_coeff = get_avg_clustering_coefficient(network_obj=a.network, mode='zero', class_name=class_name, group_names=[min_label, maj_label])\n",
    "                min_aspl, maj_aspl = get_avg_sp_lengths(network_obj=a.network, mode='out', class_name=class_name, group_names=[min_label, maj_label])\n",
    "\n",
    "            g_pre = nx_to_ig(a.network)\n",
    "\n",
    "            \n",
    "            _ = a.get_group_betweenness(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_closeness(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_eigenvector_centrality(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_degree_centrality(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_pagerank(g_pre, is_pre=True, verbose=verbose)\n",
    "\n",
    "        if remove_zero_degree_nodes:\n",
    "            if len(a.to_remove) > 0:\n",
    "                a.add_nodes()\n",
    "        \n",
    "        # if not salsa_new:\n",
    "        #     _ = a.sample_and_delete_edges(num_to_sample=num_to_sample, remove_edges=delete_edges)\n",
    "\n",
    "\n",
    "        \n",
    "        salsa_edges_per_node, salsa_all_edges, edge_type, avg_frac_min = a.reconstruct_edges(strategy=strategy, \n",
    "                                            salsa_new=salsa_new, run=i, wtf=wtf, to_pick=to_pick, consider_all_nodes=consider_all_nodes, \n",
    "                                            num_to_consider=num_to_consider, add_top_k=add_top_k, delete_edges=delete_edges,\n",
    "                                            network_file=network_file, network_obj=network_obj, class_name=class_name,\n",
    "                                            intervene=intervene, network=network_obj, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode)\n",
    "        \n",
    "\n",
    "        edge_type_dict[i] = edge_type\n",
    "        frac_mins_list.append(avg_frac_min)\n",
    "        \n",
    "        if remove_zero_degree_nodes:\n",
    "            _ = a.remove_zero_degree()\n",
    "\n",
    "        min_ccoeff_post, maj_ccoeff_post = get_avg_clustering_coefficient(network_obj=a.network, mode='zero', class_name=class_name, group_names=[min_label, maj_label])\n",
    "        min_aspl_post, maj_aspl_post = get_avg_sp_lengths(network_obj=a.network, mode='out', class_name=class_name, group_names=[min_label, maj_label])\n",
    "\n",
    "        changes_dict_maj['c_coeff'].append(np.round((maj_ccoeff_post/maj_c_coeff-1)*100, round))\n",
    "        changes_dict_min['c_coeff'].append(np.round((min_ccoeff_post/min_c_coeff-1)*100, round))\n",
    "\n",
    "        changes_dict_maj['aspl'].append(np.round((maj_aspl_post/maj_aspl-1)*100, round))\n",
    "        changes_dict_min['aspl'].append(np.round((min_aspl_post/min_aspl-1)*100, round))\n",
    "\n",
    "        g_post = nx_to_ig(a.network)\n",
    "\n",
    "        _ = a.get_group_betweenness(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_closeness(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_eigenvector_centrality(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_degree_centrality(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_pagerank(g_post, is_pre=False, verbose=verbose)\n",
    "        \n",
    "        bc_vals = a.metrics_dict['bc']\n",
    "        changes_dict_maj['bc'].append(np.round((bc_vals['post']['maj']/bc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['bc'].append(np.round((bc_vals['post']['min']/bc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        cc_vals = a.metrics_dict['cc']\n",
    "        changes_dict_maj['cc'].append(np.round((cc_vals['post']['maj']/cc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['cc'].append(np.round((cc_vals['post']['min']/cc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        evc_vals = a.metrics_dict['evc']\n",
    "        norm_evc_min_pre = evc_vals['pre']['min'] / (evc_vals['pre']['min'] + evc_vals['pre']['maj'])\n",
    "        norm_evc_maj_pre = evc_vals['pre']['maj'] / (evc_vals['pre']['min'] + evc_vals['pre']['maj'])\n",
    "\n",
    "        norm_evc_min_post = evc_vals['post']['min'] / (evc_vals['post']['min'] + evc_vals['post']['maj'])\n",
    "        norm_evc_maj_post = evc_vals['post']['maj'] / (evc_vals['post']['min'] + evc_vals['post']['maj'])\n",
    "\n",
    "        changes_dict_maj['evc_norm'].append(np.round((norm_evc_maj_post/norm_evc_maj_pre-1)*100, round))\n",
    "        changes_dict_min['evc_norm'].append(np.round((norm_evc_min_post/norm_evc_min_pre-1)*100, round))\n",
    "\n",
    "        changes_dict_maj['evc'].append(np.round((evc_vals['post']['maj']/evc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['evc'].append(np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        # changes_dict_maj['evc_abs'].append(np.round((, round))\n",
    "        # changes_dict_min['evc_abs'].append(np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        dc_vals = a.metrics_dict['dc']\n",
    "        changes_dict_maj['dc'].append(np.round((dc_vals['post']['maj']/dc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['dc'].append(np.round((dc_vals['post']['min']/dc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        pr_vals = a.metrics_dict['pr']\n",
    "        changes_dict_maj['pr'].append(np.round((pr_vals['post']['maj']/pr_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['pr'].append(np.round((pr_vals['post']['min']/pr_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        recommended_nodes_dict[i] = salsa_all_edges\n",
    "\n",
    "        print(f'Finished run {i}')\n",
    "\n",
    "    # return changes_dict_maj, changes_dict_min\n",
    "    avg_bc_change_maj, sd_bc_maj = np.round(np.average(changes_dict_maj['bc']), round), np.round(np.std(changes_dict_maj['bc']), round)\n",
    "    avg_bc_change_min, sd_bc_min = np.round(np.average(changes_dict_min['bc']), round), np.round(np.std(changes_dict_min['bc']), round)\n",
    "\n",
    "    avg_cc_change_maj, sd_cc_maj = np.round(np.average(changes_dict_maj['cc']), round), np.round(np.std(changes_dict_maj['cc']), round)\n",
    "    avg_cc_change_min, sd_cc_min = np.round(np.average(changes_dict_min['cc']), round), np.round(np.std(changes_dict_min['cc']), round)\n",
    "\n",
    "    avg_evcn_change_maj, sd_evcn_maj = np.round(np.average(changes_dict_maj['evc_norm']), round), np.round(np.std(changes_dict_maj['evc_norm']), round)\n",
    "    avg_evcn_change_min, sd_evcn_min = np.round(np.average(changes_dict_min['evc_norm']), round), np.round(np.std(changes_dict_min['evc_norm']), round)\n",
    "\n",
    "    avg_evc_change_maj, sd_evc_maj = np.round(np.average(changes_dict_maj['evc']), round), np.round(np.std(changes_dict_maj['evc']), round)\n",
    "    avg_evc_change_min, sd_evc_min = np.round(np.average(changes_dict_min['evc']), round), np.round(np.std(changes_dict_min['evc']), round)\n",
    "\n",
    "    avg_dc_change_maj, sd_dc_maj = np.round(np.average(changes_dict_maj['dc']), round), np.round(np.std(changes_dict_maj['dc']), round)\n",
    "    avg_dc_change_min, sd_dc_min = np.round(np.average(changes_dict_min['dc']), round), np.round(np.std(changes_dict_min['dc']), round)\n",
    "\n",
    "    avg_pr_change_maj, sd_pr_maj = np.round(np.average(changes_dict_maj['pr']), round), np.round(np.std(changes_dict_maj['pr']), round)\n",
    "    avg_pr_change_min, sd_pr_min = np.round(np.average(changes_dict_min['pr']), round), np.round(np.std(changes_dict_min['pr']), round)\n",
    "\n",
    "    avg_ccoeff_change_maj, sd_ccoeff_maj = np.round(np.average(changes_dict_maj['c_coeff']), round), np.round(np.std(changes_dict_maj['c_coeff']), round)\n",
    "    avg_ccoeff_change_min, sd_ccoeff_min = np.round(np.average(changes_dict_min['c_coeff']), round), np.round(np.std(changes_dict_min['c_coeff']), round)\n",
    "\n",
    "    avg_aspl_change_maj, sd_aspl_maj = np.round(np.average(changes_dict_maj['aspl']), round), np.round(np.std(changes_dict_maj['aspl']), round)\n",
    "    avg_aspl_change_min, sd_aspl_min = np.round(np.average(changes_dict_min['aspl']), round), np.round(np.std(changes_dict_min['aspl']), round)\n",
    "\n",
    "    mm = []\n",
    "    mM = []\n",
    "    MM = []\n",
    "    Mm = []\n",
    "\n",
    "    if network_file == '':\n",
    "            network = network_obj\n",
    "    else:\n",
    "            network = read_pkl_graph(network_file)\n",
    "\n",
    "    for run in recommended_nodes_dict.keys():\n",
    "\n",
    "        edge_type = {'mm': 0,\n",
    "            'MM': 0,\n",
    "            'mM': 0,\n",
    "            'Mm': 0}\n",
    "\n",
    "        for edge in recommended_nodes_dict[run]:\n",
    "            class_k = network.nodes()[edge[0]][class_name]\n",
    "            class_j = network.nodes()[edge[-1]][class_name]\n",
    "            \n",
    "            if class_k == 1 and class_j == 1:\n",
    "                edge_type['mm'] += 1\n",
    "            elif class_k == 1 and class_j == 0:\n",
    "                edge_type['mM'] += 1\n",
    "            elif class_k == 0 and class_j == 1:\n",
    "                edge_type['Mm'] += 1\n",
    "            elif class_k == 0 and class_j == 0:\n",
    "                edge_type['MM'] += 1\n",
    "        \n",
    " \n",
    "        mm.append(edge_type['mm'] / len(recommended_nodes_dict[run]) * 100)\n",
    "        MM.append(edge_type['MM'] / len(recommended_nodes_dict[run]) * 100)\n",
    "        mM.append(edge_type['mM'] / len(recommended_nodes_dict[run]) * 100)\n",
    "        Mm.append(edge_type['Mm'] / len(recommended_nodes_dict[run]) * 100)\n",
    "\n",
    "    # for run in edge_type_dict.keys():\n",
    "    #     mm.append(edge_type_dict[run]['mm'])\n",
    "    #     mM.append(edge_type_dict[run]['mM'])\n",
    "    #     MM.append(edge_type_dict[run]['MM'])\n",
    "    #     Mm.append(edge_type_dict[run]['Mm'])\n",
    "\n",
    "    # mm = [e/len(salsa_all_edges)*100 for e in mm]\n",
    "    # mM = [e/len(salsa_all_edges)*100 for e in mM]\n",
    "    # MM = [e/len(salsa_all_edges)*100 for e in MM]\n",
    "    # Mm = [e/len(salsa_all_edges)*100 for e in Mm]\n",
    "\n",
    "    avg_mm, sd_mm = np.round(np.average(mm), round), np.round(np.std(mm), round)\n",
    "    avg_mM, sd_mM = np.round(np.average(mM), round), np.round(np.std(mM), round)\n",
    "    avg_MM, sd_MM = np.round(np.average(MM), round), np.round(np.std(MM), round)\n",
    "    avg_Mm, sd_Mm = np.round(np.average(Mm), round), np.round(np.std(Mm), round)\n",
    "\n",
    "    # recommendations\n",
    "    avg_avg_frac_min, sd_frac_min = [], []\n",
    "    if strategy != 'random':\n",
    "        avg_avg_frac_min, sd_frac_min = np.round(np.average(frac_mins_list), round), np.round(np.std(frac_mins_list), round)\n",
    "    \n",
    "    # additions\n",
    "    avg_recomm_share_min, recomm_sd_min, avg_recomm_share_maj, recomm_sd_maj = get_recommendation_group_ratios(network_file='', network_obj=network_obj, recommended_nodes=recommended_nodes_dict, class_name=class_name)\n",
    "    # avg_recomm_share_min, recomm_sd_min, avg_recomm_share_maj, recomm_sd_maj = 0, 0, 0, 0\n",
    "    if to_txt:\n",
    "        save_as_txt = save_as[:-4] + '.txt'\n",
    "        f = open(save_as_txt, 'w')\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Mean stats and standard deviation for {n} runs\\n\")\n",
    "        f.write(\"---------------------------------------------------\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Added Nodes Min/Maj Fraction\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f'Avg. share of maj: {avg_recomm_share_maj}% ({recomm_sd_maj})\\n')\n",
    "        f.write(f'Avg. share of min: {avg_recomm_share_min}% ({recomm_sd_min})\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Top 10 Minority Fraction Recommendations\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f'Avg. minority fraction among Top 10 Recommendations: {avg_avg_frac_min}% ({sd_frac_min})\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Edge types\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f'mm: {avg_mm}% ({sd_mm})\\n')\n",
    "        f.write(f'mM: {avg_mM}% ({sd_mM})\\n')\n",
    "        f.write(f'MM: {avg_MM}% ({sd_MM})\\n')\n",
    "        f.write(f'Mm: {avg_Mm}% ({sd_Mm})\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write('DC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_dc_change_maj} ({sd_dc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_dc_change_min} ({sd_dc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('BC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_bc_change_maj} ({sd_bc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_bc_change_min} ({sd_bc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('CC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_cc_change_maj} ({sd_cc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_cc_change_min} ({sd_cc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('EVC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_evc_change_maj} ({sd_evc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_evc_change_min} ({sd_evc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Norm EVC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_evcn_change_maj} ({sd_evcn_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_evcn_change_min} ({sd_evcn_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('PR Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_pr_change_maj} ({sd_pr_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_pr_change_min} ({sd_pr_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Clustering Coeff Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_ccoeff_change_maj} ({sd_ccoeff_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_ccoeff_change_min} ({sd_ccoeff_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Avg. Shortest Path Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_aspl_change_maj} ({sd_aspl_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_aspl_change_min} ({sd_aspl_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        # f.write(f\"Avg. overlap: {np.round(np.average(overlaps), round)}\\n\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Mean stats and standard deviation for {n} runs\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print('Added Nodes Min/Maj Fraction')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'Avg. share of maj: {avg_recomm_share_maj}% ({recomm_sd_maj})')\n",
    "    print(f'Avg. share of min: {avg_recomm_share_min}% ({recomm_sd_min})')\n",
    "    print(\"\")\n",
    "    print('Edge types')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'mm: {avg_mm}% ({sd_mm})')\n",
    "    print(f'mM: {avg_mM}% ({sd_mM})')\n",
    "    print(f'MM: {avg_MM}% ({sd_MM})')\n",
    "    print(f'Mm: {avg_Mm}% ({sd_Mm})')\n",
    "    print(\"\")\n",
    "    print('Top 10 Minority Fraction Recommendations')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'Avg. minority fraction: {avg_avg_frac_min}% ({sd_frac_min})')\n",
    "    print(\"\")\n",
    "    print('DC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_dc_change_maj} ({sd_dc_maj})\")\n",
    "    print(f\"Minority: {avg_dc_change_min} ({sd_dc_min})\")\n",
    "    print(\"\")\n",
    "    print('BC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_bc_change_maj} ({sd_bc_maj})\")\n",
    "    print(f\"Minority: {avg_bc_change_min} ({sd_bc_min})\")\n",
    "    print(\"\")\n",
    "    print('CC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_cc_change_maj} ({sd_cc_maj})\")\n",
    "    print(f\"Minority: {avg_cc_change_min} ({sd_cc_min})\")\n",
    "    print(\"\")\n",
    "    print('EVC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_evc_change_maj} ({sd_evc_maj})\")\n",
    "    print(f\"Minority: {avg_evc_change_min} ({sd_evc_min})\")\n",
    "    print(\"\")\n",
    "    print('Norm EVC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_evcn_change_maj} ({sd_evcn_maj})\")\n",
    "    print(f\"Minority: {avg_evcn_change_min} ({sd_evcn_min})\")\n",
    "    print(\"\")\n",
    "    print('PR Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_pr_change_maj} ({sd_pr_maj})\")\n",
    "    print(f\"Minority: {avg_pr_change_min} ({sd_pr_min})\")\n",
    "    print(\"\")\n",
    "    print('Clustering Coeff Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_ccoeff_change_maj} ({sd_ccoeff_maj})\")\n",
    "    print(f\"Minority: {avg_ccoeff_change_min} ({sd_ccoeff_min})\")\n",
    "    print(\"\")\n",
    "    print('Avg. Shortest Path Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_aspl_change_maj} ({sd_aspl_maj})\")\n",
    "    print(f\"Minority: {avg_aspl_change_min} ({sd_aspl_min})\")\n",
    "    print(\"\")\n",
    "    # print(f\"Avg. overlap: {np.round(np.average(overlaps), round)}\")\n",
    "\n",
    "    results = {'pct_changes': {'min': changes_dict_min, 'maj': changes_dict_maj}, \n",
    "               'frac_mins': [frac_mins_list, sd_frac_min], 'edge_types': edge_type_dict, \n",
    "               'recommended_nodes': recommended_nodes_dict}\n",
    "    if save_results:\n",
    "        \n",
    "        with open(save_as, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print('')\n",
    "        print(f'Results saved to disk as {save_as}')\n",
    "\n",
    "    return results\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_group_ratios(network_file='', network_obj=None, recommended_nodes=[], class_name='m'):\n",
    "    \n",
    "    if network_file == '':\n",
    "        network = network_obj\n",
    "    else:\n",
    "        network = read_pkl_graph(network_file)\n",
    "\n",
    "    all_nodes = network.nodes()\n",
    "    shares_min = []\n",
    "    shares_maj = []\n",
    "\n",
    "\n",
    "    for i in recommended_nodes.keys():\n",
    "\n",
    "        count_maj = 0\n",
    "        count_min = 0\n",
    "       \n",
    "\n",
    "        for tup in recommended_nodes[i]:\n",
    "            node = tup[-1]\n",
    "            if all_nodes[node][class_name] == 0:\n",
    "                count_maj += 1\n",
    "            elif all_nodes[node][class_name] == 1:\n",
    "                count_min += 1\n",
    "\n",
    "        shares_min.append(count_min/len(recommended_nodes[i])*100)\n",
    "        shares_maj.append(count_maj/len(recommended_nodes[i])*100)\n",
    "\n",
    "    avg_recomm_share_min, recomm_sd_min = np.round(np.average(shares_min), 5), np.round(np.std(shares_min), 5)\n",
    "    avg_recomm_share_maj, recomm_sd_maj = np.round(np.average(shares_maj), 5), np.round(np.std(shares_maj), 5)\n",
    "\n",
    "    \n",
    "    return avg_recomm_share_min, recomm_sd_min, avg_recomm_share_maj, recomm_sd_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_clustering_coefficient(network_file='', network_obj=None, mode='zero', class_name='m', group_names=[1, 0], verbose=False):\n",
    "\n",
    "    def get_nodes_per_class(network, class_name='m', group_names=[1, 0]):\n",
    "        \n",
    "        # class_\n",
    "        if class_name == 'm':\n",
    "            min_nodes_ig = network.vs.select(m=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(m=group_names[-1])\n",
    "\n",
    "        elif class_name == 'class_':\n",
    "            min_nodes_ig = network.vs.select(class_=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(class_=group_names[-1])\n",
    "\n",
    "        elif class_name == 'age':\n",
    "            min_nodes_ig = network.vs.select(age=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(age=group_names[-1])\n",
    "\n",
    "        return min_nodes_ig, maj_nodes_ig\n",
    "    \n",
    "    if network_file == '':\n",
    "        network = network_obj\n",
    "    else:\n",
    "        network = read_pkl_graph(network_file)\n",
    "\n",
    "    network_ig = nx_to_ig(network)\n",
    "\n",
    "    min_nodes, maj_nodes = get_nodes_per_class(network_ig, class_name=class_name, group_names=group_names)\n",
    "\n",
    "    # mode: how to treat nodes with degree < 2\n",
    "    transitivity = network_ig.transitivity_local_undirected(mode=mode)\n",
    "    network_ig.vs['clustering_coeff'] = transitivity\n",
    "\n",
    "    min_clustering_coeff = np.average(np.array(min_nodes.get_attribute_values('clustering_coeff')))\n",
    "    maj_clustering_coeff = np.average(np.array(maj_nodes.get_attribute_values('clustering_coeff')))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Min clustering coeff: {min_clustering_coeff}')\n",
    "        print(f'Maj clustering coeff: {maj_clustering_coeff}')\n",
    "\n",
    "    return min_clustering_coeff, maj_clustering_coeff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sp_lengths(network_file='', network_obj=None, mode='out', class_name='m', group_names=[1, 0], verbose=False):\n",
    "\n",
    "    def get_nodes_per_class(network, class_name='m', group_names=['young', 'old']):\n",
    "        \n",
    "        # class_\n",
    "        if class_name == 'm':\n",
    "            min_nodes_ig = network.vs.select(m=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(m=group_names[-1])\n",
    "\n",
    "        elif class_name == 'class_':\n",
    "            min_nodes_ig = network.vs.select(class_=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(class_=group_names[-1])\n",
    "\n",
    "        elif class_name == 'age':\n",
    "            min_nodes_ig = network.vs.select(age=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(age=group_names[-1])\n",
    "\n",
    "        return min_nodes_ig, maj_nodes_ig\n",
    "    \n",
    "    if network_file == '':\n",
    "        network = network_obj\n",
    "    else:\n",
    "        network = read_pkl_graph(network_file)\n",
    "    network_ig = nx_to_ig(network)\n",
    "\n",
    "    shortest_path_lengths = network_ig.shortest_paths(mode=mode)\n",
    "    arr = np.array(shortest_path_lengths)\n",
    "    arr[arr==inf] = 0\n",
    "    avg_sp_lens = [np.average(lens) for lens in arr]\n",
    "\n",
    "    min_nodes, maj_nodes = get_nodes_per_class(network_ig, class_name=class_name, group_names=group_names)\n",
    "\n",
    "    network_ig.vs['avg_sp_len'] = avg_sp_lens\n",
    "\n",
    "    min_avg_sp = np.average(np.array(min_nodes.get_attribute_values('avg_sp_len')))\n",
    "    maj_avg_sp = np.average(np.array(maj_nodes.get_attribute_values('avg_sp_len')))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Min avg sp length: {min_avg_sp}')\n",
    "        print(f'Maj avg sp length: {maj_avg_sp}')\n",
    "\n",
    "    return min_avg_sp, maj_avg_sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist_per_group(G, attribute='', group_names=['', ''], group_labels=[1, 0]):\n",
    "    minority = [G.degree(n[0]) for n in G.nodes(data=True) if n[-1][attribute] == group_labels[0]]\n",
    "    majority = [G.degree(n[0]) for n in G.nodes(data=True) if n[-1][attribute] == group_labels[1]]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, sharey=True)\n",
    "    fig.suptitle('Degree distribution by group')\n",
    "    \n",
    "    sns.histplot(ax=ax[0], data=minority)\n",
    "    # ax[0].set_ylabel('test')\n",
    "    ax[0].set_xlabel('Degree')\n",
    "    ax[0].set_title(f'{group_names[0]} (minority)')\n",
    "\n",
    "    sns.histplot(ax=ax[1], data=majority)\n",
    "    ax[1].set_xlabel('Degree')\n",
    "    ax[1].set_title(f'{group_names[1]} (majority)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_ratios(network_obj, attribute='', group_names=['', ''], group_labels=[1, 0]):\n",
    "\n",
    "    minority = [n for n in network_obj.nodes(data=True) if n[-1][attribute] == group_labels[0]]\n",
    "    majority = [n for n in network_obj.nodes(data=True) if n[-1][attribute] == group_labels[1]]\n",
    "\n",
    "    print(f'# {group_names[0]} nodes (minority): {len(minority)} ({np.round(len(minority)/(len(minority)+len(majority))*100, 2)}%)')\n",
    "    print(f'# {group_names[1]} nodes (majority): {len(majority)} ({np.round(len(majority)/(len(minority)+len(majority))*100, 2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    sns.histplot(degrees)\n",
    "    plt.xlabel('Degree')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_vnum(network):\n",
    "    print(f'Number of edges: {network.number_of_edges()}')\n",
    "    print(f'Number of nodes: {network.number_of_nodes()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_nodes(network, name_as_str=True):\n",
    "\n",
    "    old = list(network.nodes())\n",
    "    new = list(range(0, len(old)))\n",
    "\n",
    "    if name_as_str:\n",
    "        old_str = [str(i) for i in old]\n",
    "        mapping1 = dict(zip(old, old_str))\n",
    "    else:\n",
    "        mapping1 = dict(zip(old, old))\n",
    "    nx.set_node_attributes(network, values=mapping1, name='name')\n",
    "\n",
    "    mapping2 = dict(zip(old, new))\n",
    "\n",
    "    return nx.relabel_nodes(network, mapping2, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hubs_auths_analysis(network_obj, ha_dict):\n",
    "\n",
    "    mins_hubs = []\n",
    "    mins_auths = []\n",
    "\n",
    "    majs_hubs = []\n",
    "    majs_auths = []\n",
    "\n",
    "    for i, dic in enumerate(ha_dict):\n",
    "        node_cls = network_obj.nodes()[i]['m']\n",
    "        \n",
    "        hubs_cls = [network_obj.nodes()[hub]['m'] for hub in dic[i]['hubs']]\n",
    "        frac_min_hubs = sum(hubs_cls) / len(hubs_cls)\n",
    "\n",
    "        auths_cls = [network_obj.nodes()[auth]['m'] for auth in dic[i]['auths']]\n",
    "        frac_min_auths = sum(auths_cls) / len(auths_cls)\n",
    "\n",
    "        if node_cls == 1:\n",
    "            mins_hubs.append(frac_min_hubs)\n",
    "            mins_auths.append(frac_min_auths)\n",
    "        else:\n",
    "            majs_hubs.append(frac_min_hubs)\n",
    "            majs_auths.append(frac_min_auths)\n",
    "\n",
    "    print(f'Avg. frac. min. among hubs if node is minority: {np.round(np.average(mins_hubs), 3)}')\n",
    "    print(f'Avg. frac. min. among authorities if node is minority: {np.round(np.average(mins_auths), 3)}')\n",
    "\n",
    "    print(f'Avg. frac. min. among hubs if node is majority: {np.round(np.average(majs_hubs), 3)}')\n",
    "    print(f'Avg. frac. min. among authorities if node is majority: {np.round(np.average(majs_auths), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendation_analysis(wtf_scores, network):\n",
    "    in_degs = []\n",
    "    for arr in wtf_scores:\n",
    "        in_deg = []\n",
    "        for node in arr:\n",
    "            in_deg.append(network.in_degree(node))\n",
    "        in_degs.append(in_deg)\n",
    "    \n",
    "    avg_med_indegs = [(np.average(arr), np.median(arr)) for arr in in_degs]\n",
    "\n",
    "    total_avg = np.average([tup[0] for tup in avg_med_indegs])\n",
    "    total_median = np.median([tup[1] for tup in avg_med_indegs])\n",
    "\n",
    "    all_indegs = [network.in_degree(node) for node in network.nodes()]\n",
    "\n",
    "    avg_all_indegs = np.average(all_indegs)\n",
    "    median_all_indegs = np.median(all_indegs)\n",
    "\n",
    "    print('Recommendations in-degree')\n",
    "    print('---------------------------')\n",
    "    print(f'Avg: {total_avg}')\n",
    "    print(f'Median: {total_median}')\n",
    "    print('')\n",
    "    print('Overall network in-degree')\n",
    "    print('---------------------------')\n",
    "    print(f'Avg: {avg_all_indegs}')\n",
    "    print(f'Median: {median_all_indegs}')\n",
    "\n",
    "    return {'recomms': (total_avg, total_median), 'network': (avg_all_indegs, median_all_indegs)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "network82 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82 = reindex_nodes(network82)\n",
    "wtf_scores = who_to_follow_rank(A=nx.adjacency_matrix(network82), njobs=-1)\n",
    "a82 = recommendation_analysis(wtf_scores, network82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network82 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82 = reindex_nodes(network82)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(network82), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(network82, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "network82_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "network82_results_2000nodes_final = run_n_simulations(network_obj=network82_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx82_2000nodes_final.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative strategy\n",
    "\n",
    "network82_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "network82_results_2000nodes_new = run_n_simulations(network_obj=network82_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx82_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "network82_topk = nx.read_gpickle(config.ROOT + \"nx_DPAH_10000_0208_0802.pkl\")\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "\n",
    "network82_results_2000nodes = run_n_simulations(network_obj=network82_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='82_base_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "network82_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network82_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network82_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network82_results_2000nodes = run_n_simulations(network_obj=network82_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='82_new_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network88 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88 = reindex_nodes(network88)\n",
    "wtf_scores_88 = who_to_follow_rank(A=nx.adjacency_matrix(network88), njobs=-1)\n",
    "\n",
    "a88 = recommendation_analysis(wtf_scores_88, network88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network88 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88 = reindex_nodes(network88)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(network88), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(network88, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "network88_results_2000nodes_final = run_n_simulations(network_obj=network88_topk, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx88_2000nodes_final.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative strategy\n",
    "\n",
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "network88_results_2000nodes_new = run_n_simulations(network_obj=network88_topk, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx88_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "\n",
    "network88_results_2000nodes = run_n_simulations(network_obj=network88_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='88_base_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network88_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network88_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network88_results_2000nodes = run_n_simulations(network_obj=network88_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='88_new_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network28 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28 = reindex_nodes(network28)\n",
    "wtf_scores_28 = who_to_follow_rank(A=nx.adjacency_matrix(network28), njobs=-1)\n",
    "\n",
    "a28 = recommendation_analysis(wtf_scores_28, network28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network28 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28 = reindex_nodes(network28)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(network28), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(network28, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "network28_results_2000nodes_final = run_n_simulations(network_obj=network28_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx28_2000nodes_final.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative strategy\n",
    "\n",
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "network28_results_2000nodes_new = run_n_simulations(network_obj=network28_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx28_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "\n",
    "network28_results_2000nodes = run_n_simulations(network_obj=network28_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='28_base_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network28_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network28_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network28_results_2000nodes = run_n_simulations(network_obj=network28_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='28_new_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network22 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22 = reindex_nodes(network22)\n",
    "wtf_scores_22 = who_to_follow_rank(A=nx.adjacency_matrix(network22), njobs=-1)\n",
    "\n",
    "a22 = recommendation_analysis(wtf_scores_22, network22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network22 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22 = reindex_nodes(network22)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(network22), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(network22, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "network22_results_2000nodes_final = run_n_simulations(network_obj=network22_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx22_2000nodes_final.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative strategy\n",
    "\n",
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "network22_results_2000nodes_new = run_n_simulations(network_obj=network22_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx22_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "\n",
    "network22_results_2000nodes = run_n_simulations(network_obj=network22_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='22_base_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network22_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network22_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network22_results_2000nodes = run_n_simulations(network_obj=network22_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='22_new_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network55 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55 = reindex_nodes(network55)\n",
    "wtf_scores_55 = who_to_follow_rank(A=nx.adjacency_matrix(network55), njobs=-1)\n",
    "\n",
    "a55 = recommendation_analysis(wtf_scores_55, network55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network55 = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55 = reindex_nodes(network55)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(network55), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(network55, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "network55_results_2000nodes_final = run_n_simulations(network_obj=network55_topk, n=10, \n",
    "                                            elete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx55_2000nodes_final.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative strategy\n",
    "\n",
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "network55_results_2000nodes_new = run_n_simulations(network_obj=network55_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx55_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "\n",
    "network55_results_2000nodes = run_n_simulations(network_obj=network55_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='55_base_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network55_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network55_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network55_results_2000nodes = run_n_simulations(network_obj=network55_topk, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='55_new_ix.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blogs network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19025"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "blogs.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. frac. min. among hubs if node is minority: 0.866\n",
      "Avg. frac. min. among authorities if node is minority: 0.728\n",
      "Avg. frac. min. among hubs if node is majority: 0.256\n",
      "Avg. frac. min. among authorities if node is majority: 0.316\n"
     ]
    }
   ],
   "source": [
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "blogs = reindex_nodes(blogs)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(blogs), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(blogs, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "blogs = reindex_nodes(blogs)\n",
    "\n",
    "blogs_results = run_n_simulations(network_obj=blogs, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='blogs_new.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=220, add_top_k=True,\n",
    "                                            intervene=False, recomms=None\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "blogs = reindex_nodes(blogs)\n",
    "\n",
    "blogs_results = run_n_simulations(network_obj=blogs, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='blogs_new.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=220, add_top_k=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "blogs = reindex_nodes(blogs)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=blogs, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(blogs), to_add=[top_min_evc, top_maj_evc], p=0.85, top=10, num_cores=-1, network_obj=blogs)\n",
    "\n",
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "blogs = reindex_nodes(blogs)\n",
    "blogs_results_ix = run_n_simulations(network_obj=blogs, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='blogs_ix_evc_both.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=220, add_top_k=True,\n",
    "                                            recomms=results\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hate network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")\n",
    "hate.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. frac. min. among hubs if node is minority: 0.513\n",
      "Avg. frac. min. among authorities if node is minority: 0.558\n",
      "Avg. frac. min. among hubs if node is majority: 0.139\n",
      "Avg. frac. min. among authorities if node is majority: 0.236\n"
     ]
    }
   ],
   "source": [
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")\n",
    "hate = reindex_nodes(hate)\n",
    "ha_dict = who_to_follow_rank(A=nx.adjacency_matrix(hate), njobs=-1)\n",
    "\n",
    "hubs_auths_analysis(hate, ha_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "\n",
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")\n",
    "hate = reindex_nodes(hate)\n",
    "\n",
    "hate_results = run_n_simulations(network_obj=hate, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='hate_new.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=1000, add_top_k=True,\n",
    "                                            intervene=False, recomms=None\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #1\n",
    "\n",
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")\n",
    "hate = reindex_nodes(hate)\n",
    "\n",
    "hate_results = run_n_simulations(network_obj=hate, n=10, \n",
    "                                            delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='intervention', save_results=True, save_as='hate_new.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=1000, add_top_k=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intervention #2\n",
    "\n",
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")\n",
    "hate = reindex_nodes(hate)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=hate, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(hate), to_add=[top_min_evc, top_maj_evc], p=0.85, top=10, num_cores=-1, network_obj=hate)\n",
    "\n",
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")\n",
    "hate = reindex_nodes(hate)\n",
    "hate_results_ix = run_n_simulations(network_obj=hate, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='hate_ix_evc_both.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=1000, add_top_k=True,\n",
    "                                            recomms=results\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c356627e19890640c661bf35b313e3ac05771087fe3f4aaa57d60dc61ec63b4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
