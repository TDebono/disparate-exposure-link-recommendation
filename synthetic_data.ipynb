{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import igraph as ig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from synthetic_networks.random_network import *\n",
    "from synthetic_networks.bba import *\n",
    "from synthetic_networks.bba_homophily import *\n",
    "from synthetic_networks.dpah import *\n",
    "import itertools\n",
    "from data_loader import *\n",
    "from fast_pagerank import pagerank_power\n",
    "import random\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from numpy import inf\n",
    "\n",
    "import os\n",
    "import powerlaw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from joblib import delayed\n",
    "from joblib import Parallel\n",
    "from collections import Counter\n",
    "from fast_pagerank import pagerank_power\n",
    "\n",
    "\n",
    "# sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g82 = DPAH(N=40, \n",
    "             fm=0.2, \n",
    "             d=0.1,\n",
    "             plo_M=2.5, \n",
    "             plo_m=2.5, \n",
    "             h_MM=0.8, \n",
    "             h_mm=0.2, \n",
    "             verbose=False)\n",
    "z = remove_zero_degree(g82)\n",
    "class_labels = nx.get_node_attributes(g82, 'm')\n",
    "color_map = {1:'#446df6', 0: '#93032e'}\n",
    "# pos = nx.spring_layout(g82, iterations=10, scale=2)\n",
    "pos = nx.random_layout(g82)\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G=g82,\n",
    "    pos=pos,\n",
    "    labels=class_labels,\n",
    "    with_labels=False,\n",
    "    alpha=0.8,\n",
    "    node_size=30,\n",
    "    arrowsize=5,\n",
    "    width=0.1,\n",
    "    node_color=[color_map[g82.nodes()[node]['m']] for node in g82],\n",
    ")\n",
    "plt.title(\"(d) Homophilic majority / heterophilic minority\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g28 = DPAH(N=40, \n",
    "             fm=0.2, \n",
    "             d=0.1,\n",
    "             plo_M=2.5, \n",
    "             plo_m=2.5, \n",
    "             h_MM=0.2, \n",
    "             h_mm=0.8, \n",
    "             verbose=False)\n",
    "z = remove_zero_degree(g28)\n",
    "class_labels = nx.get_node_attributes(g28, 'm')\n",
    "color_map = {1:'#446df6', 0: '#93032e'}\n",
    "# pos = nx.spring_layout(g82, iterations=10, scale=2)\n",
    "pos = nx.random_layout(g28)\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G=g28,\n",
    "    pos=pos,\n",
    "    labels=class_labels,\n",
    "    with_labels=False,\n",
    "    alpha=0.8,\n",
    "    node_size=30,\n",
    "    arrowsize=5,\n",
    "    width=0.1,\n",
    "    node_color=[color_map[g28.nodes()[node]['m']] for node in g28]\n",
    ")\n",
    "plt.title(\"(b) Heterophilic majority / homophilic minority\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g22 = DPAH(N=40, \n",
    "             fm=0.2, \n",
    "             d=0.1,\n",
    "             plo_M=2.5, \n",
    "             plo_m=2.5, \n",
    "             h_MM=0.2, \n",
    "             h_mm=0.2, \n",
    "             verbose=False)\n",
    "z = remove_zero_degree(g22)\n",
    "class_labels = nx.get_node_attributes(g22, 'm')\n",
    "color_map = {1:'#446df6', 0: '#93032e'}\n",
    "# pos = nx.spring_layout(g82, iterations=10, scale=2)\n",
    "pos = nx.random_layout(g22)\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G=g22,\n",
    "    pos=pos,\n",
    "    labels=class_labels,\n",
    "    with_labels=False,\n",
    "    alpha=0.8,\n",
    "    node_size=30,\n",
    "    arrowsize=5,\n",
    "    width=0.1,\n",
    "    node_color=[color_map[g22.nodes()[node]['m']] for node in g22]\n",
    ")\n",
    "plt.title(\"(a) Heterophilic majority / heterophilic minority\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g55 = DPAH(N=40, \n",
    "             fm=0.2, \n",
    "             d=0.1,\n",
    "             plo_M=2.5, \n",
    "             plo_m=2.5, \n",
    "             h_MM=0.2, \n",
    "             h_mm=0.2, \n",
    "             verbose=False)\n",
    "z = remove_zero_degree(g55)\n",
    "class_labels = nx.get_node_attributes(g55, 'm')\n",
    "color_map = {1:'#446df6', 0: '#93032e'}\n",
    "# pos = nx.spring_layout(g82, iterations=10, scale=2)\n",
    "pos = nx.random_layout(g55)\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G=g55,\n",
    "    pos=pos,\n",
    "    labels=class_labels,\n",
    "    with_labels=False,\n",
    "    alpha=0.8,\n",
    "    node_size=30,\n",
    "    arrowsize=5,\n",
    "    width=0.1,\n",
    "    node_color=[color_map[g55.nodes()[node]['m']] for node in g55]\n",
    ")\n",
    "plt.title(\"(c) Neutral majority / neutral minority\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g88 = DPAH(N=40, \n",
    "             fm=0.2, \n",
    "             d=0.1,\n",
    "             plo_M=2.5, \n",
    "             plo_m=2.5, \n",
    "             h_MM=0.2, \n",
    "             h_mm=0.2, \n",
    "             verbose=False)\n",
    "z = remove_zero_degree(g88)\n",
    "class_labels = nx.get_node_attributes(g88, 'm')\n",
    "color_map = {1:'#446df6', 0: '#93032e'}\n",
    "# pos = nx.spring_layout(g82, iterations=10, scale=2)\n",
    "pos = nx.random_layout(g88)\n",
    "\n",
    "nx.draw_networkx(\n",
    "    G=g88,\n",
    "    pos=pos,\n",
    "    labels=class_labels,\n",
    "    with_labels=False,\n",
    "    alpha=0.8,\n",
    "    node_size=30,\n",
    "    arrowsize=5,\n",
    "    width=0.1,\n",
    "    node_color=[color_map[g88.nodes()[node]['m']] for node in g88]\n",
    ")\n",
    "plt.title(\"(e) Homophilic majority / Homophilic minority\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 5)\n",
    "# ax[0] = nx.draw_networkx(\n",
    "#     G=g28,\n",
    "#     pos=nx.random_layout(g28),\n",
    "#     labels=class_labels,\n",
    "#     with_labels=False,\n",
    "#     alpha=0.8,\n",
    "#     node_size=30,\n",
    "#     arrowsize=5,\n",
    "#     width=0.1,\n",
    "#     node_color=[color_map[g28.nodes()[node]['m']] for node in g28]\n",
    "# )\n",
    "\n",
    "# # ax[1] = nx.draw_networkx(\n",
    "# #     G=g82,\n",
    "# #     pos=nx.random_layout(g82),\n",
    "# #     labels=class_labels,\n",
    "# #     with_labels=False,\n",
    "# #     alpha=0.8,\n",
    "# #     node_size=30,\n",
    "# #     arrowsize=5,\n",
    "# #     width=0.1,\n",
    "# #     node_color=[color_map[g82.nodes()[node]['m']] for node in g82]\n",
    "# # )\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nodes, min-maj split, homophily maj, homophily min\n",
    "nx.write_gpickle(G, config.ROOT+'dummy100_nx_DPAH_0802.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIGNET = 11000\n",
    "EXT = '.gpickle'\n",
    "TOPK = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ppr(node_index, A, p, top):\n",
    "    pp = np.zeros(A.shape[0])\n",
    "    pp[node_index] = A.shape[0]\n",
    "    pr = pagerank_power(A, p=p, personalize=pp)\n",
    "    pr = pr.argsort()[-top-1:][::-1]\n",
    "    #time.sleep(0.01)\n",
    "    return pr[pr!=node_index][:top]\n",
    "\n",
    "def get_circle_of_trust_per_node(A, p=0.85, top=TOPK, num_cores=40):\n",
    "    return Parallel(n_jobs=num_cores)(delayed(_ppr)(node_index, A, p, top) for node_index in np.arange(A.shape[0]))\n",
    "\n",
    "def frequency_by_circle_of_trust(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=-1):\n",
    "    results = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, -1)\n",
    "    unique_elements, counts_elements = np.unique(np.concatenate(results), return_counts=True)\n",
    "    del(results)\n",
    "    return [ 0 if node_index not in unique_elements else counts_elements[np.argwhere(unique_elements == node_index)[0, 0]] for node_index in np.arange(A.shape[0])]\n",
    "\n",
    "def _salsa(node_index, cot, A, top=TOPK):\n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                       and n not in cot\n",
    "                                                                                       and n != node_index\n",
    "                                                                                       and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "    del(BG)\n",
    "    #time.sleep(0.01)\n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def get_topj_bc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    bc_full = g.betweenness(directed=True)\n",
    "    g.vs['bc'] = bc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        # maj_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_bc_temp = min_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        min_bc_temp.sort()\n",
    "        topj = min_bc_temp[-j:]\n",
    "\n",
    "        inds = [min_bc.index(t) for t in topj][-j:]\n",
    "        \n",
    "    if mode == 'maj':\n",
    "        maj_bc = maj_nodes_ig.get_attribute_values('bc')\n",
    "        maj_bc_temp = maj_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        maj_bc_temp.sort()\n",
    "        topj = maj_bc_temp[-j:]\n",
    "\n",
    "        inds = [maj_bc.index(t) for t in topj][-j:]\n",
    "\n",
    "    return inds\n",
    "\n",
    "def get_topj_evc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    evc_full = g.eigenvector_centrality()\n",
    "    g.vs['evc'] = evc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_evc = min_nodes_ig.get_attribute_values('evc')\n",
    "        # maj_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_evc_temp = min_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        min_evc_temp.sort()\n",
    "        topj = min_evc_temp[-j:]\n",
    "\n",
    "        inds = [min_evc.index(t) for t in topj][-j:]\n",
    "    if mode == 'maj':\n",
    "        maj_evc = maj_nodes_ig.get_attribute_values('evc')\n",
    "        maj_evc_temp = maj_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        maj_evc_temp.sort()\n",
    "        topj = maj_evc_temp[-j:]\n",
    "\n",
    "        inds = [maj_evc.index(t) for t in topj][-j:]\n",
    "\n",
    "    return inds\n",
    "\n",
    "def DEPR_salsa_intervention(node_index, cot, A, top=TOPK, intervene=False, to_add=None, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "        \n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                       and n not in cot\n",
    "                                                                                       and n != node_index\n",
    "                                                                                       and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "    del(BG)\n",
    "    \n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def _salsa2(node_index, cot, A, top=TOPK, intervene=False, to_add=None, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "    \n",
    "    \n",
    "    \n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                       and n not in cot\n",
    "                                                                                       and n != node_index\n",
    "                                                                                       and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "    del(BG)\n",
    "    print(node_index)\n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def frequency_by_who_to_follow(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=40):\n",
    "    cot_per_node = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, -1)\n",
    "    results = Parallel(n_jobs=-1)(delayed(_salsa)(node_index, cot, A, top) for node_index, cot in enumerate(cot_per_node))\n",
    "    unique_elements, counts_elements = np.unique(np.concatenate(results), return_counts=True)\n",
    "    del(results)\n",
    "    return [0 if node_index not in unique_elements else counts_elements[np.argwhere(unique_elements == node_index)[0, 0]] for node_index in np.arange(A.shape[0])]\n",
    "\n",
    "def frequency_by_who_to_follow2(A, cot_per_node=None, p=0.85, top=TOPK, num_cores=-1, intervene=False, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "\n",
    "    if not intervene:\n",
    "        cot_per_node = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, num_cores)\n",
    "        results = Parallel(n_jobs=-1)(delayed(_salsa)(node_index, cot, A, top) for node_index, cot in enumerate(cot_per_node))\n",
    "        unique_elements, counts_elements = np.unique(np.concatenate(results), return_counts=True)\n",
    "        # del(results)\n",
    "        return results\n",
    "    \n",
    "    if intervene:\n",
    "\n",
    "        cot_per_node = cot_per_node if cot_per_node is not None else get_circle_of_trust_per_node(A, p, top, num_cores)\n",
    "        results = Parallel(n_jobs=num_cores)(delayed(_salsa_intervention)(node_index, cot, A, top=TOPK, intervene=intervene, to_add=ix, network=network, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode) for node_index, cot in enumerate(cot_per_node))\n",
    "        unique_elements, counts_elements = np.unique(np.concatenate(results), return_counts=True)\n",
    "        # del(results)\n",
    "        return results\n",
    "\n",
    "def who_to_follow_rank(A, njobs=-1, intervene=False, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "    if A.shape[0] < BIGNET:\n",
    "        return wtf_small(A=A, njobs=njobs, intervene=intervene, network=network, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode)\n",
    "    else:\n",
    "        # TODO: implement optimal (or faster) solution for big net\n",
    "        return wtf_small(A=A, njobs=njobs, intervene=intervene, network=network, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode)\n",
    "        \n",
    "def wtf_small(A, njobs, intervene=False, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "    # utils.printf('cot_per_node...')\n",
    "    cot_per_node = get_circle_of_trust_per_node(A, p=0.85, top=TOPK, num_cores=njobs)\n",
    "\n",
    "    # utils.printf('cot...')\n",
    "    # cot = frequency_by_circle_of_trust(A, cot_per_node=cot_per_node, p=0.85, top=TOPK, num_cores=njobs)\n",
    "\n",
    "    # utils.printf('wtf...')\n",
    "    wtf = frequency_by_who_to_follow2(A=A, cot_per_node=cot_per_node, p=0.85, top=TOPK, num_cores=njobs, intervene=intervene, network=network, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode)\n",
    "    return wtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twitter_wtf(network, sparse_adj, node_id, k_for_circle_of_trust=20, cot_factor= 0.05, tol=1e-8,\n",
    "                damping_factor=.85, k_for_recommendation=-1):\n",
    "    \"\"\"This method aims to realize a link prediction algorithm used by Twitter to perform\n",
    "        the WTF recommendation on the platform.\n",
    "        The algorithm can be seen at 'https://web.stanford.edu/~rezab/papers/wtf_overview.pdf'.\n",
    "        The algorithm consists of two phases:\n",
    "            1) Compute the circle of trust for the user you want to recommend(top-k nodes in PPR)\n",
    "            2) Compute the top-k nodes using score propagation\n",
    "    \"\"\"\n",
    "    k_for_circle_of_trust = min(int(network.number_of_nodes()*cot_factor), k_for_circle_of_trust)\n",
    "    #1st phase: Compute circle of trust of user according to Personalized PageRank\n",
    "    personalize = np.zeros(shape=network.number_of_nodes())\n",
    "    personalize[node_id] = 1\n",
    "    values_of_personalized_pr = pagerank_power(sparse_adj, p=damping_factor, personalize=personalize, tol=1e-6)\n",
    "    circle_of_trust = values_of_personalized_pr.argsort()[-k_for_circle_of_trust:][::-1]\n",
    "\n",
    "    #2nd phase: init bipartite graph\n",
    "    bipartite_graph = nx.DiGraph()\n",
    "    #add nodes belonging to the circle of trust as hubs(H)\n",
    "    for node in circle_of_trust:\n",
    "        #these nodes are \"hubs\"(H) in the bipartite graph\n",
    "        bipartite_graph.add_node(str(node)+\"H\")\n",
    "    #add out neighbors of nodes belonging to the circle of trust as authorities(A)\n",
    "    for node in circle_of_trust:\n",
    "        for out_neighbor in network.neighbors(node):\n",
    "            #direction is inverted for a matter of simplicity in the sequent phases\n",
    "            bipartite_graph.add_edge(str(out_neighbor)+\"A\", str(node)+\"H\")\n",
    "\n",
    "    #retrieve adjacency matrix of bipartite graph\n",
    "    A = nx.to_numpy_array(bipartite_graph)\n",
    "\n",
    "    #retrieve list of all nodes splitted by authority or hub\n",
    "    all_nodes = list(bipartite_graph.nodes())\n",
    "    hub_nodes = [int(x[:-1]) for x in all_nodes if 'H' in x]\n",
    "    authority_nodes = [int(x[:-1]) for x in all_nodes if 'A' in x]\n",
    "\n",
    "    #3rd phase: start building ingredients of our SALSA algorithm\n",
    "    #these are the transition matrices determined by the bipartite graph\n",
    "    S_prime = A[len(hub_nodes):, :][:, :len(hub_nodes)].copy()\n",
    "    R_prime = S_prime.T.copy()\n",
    "    #normalize both matrices\n",
    "    denominator_S_prime = S_prime.sum(axis=0)\n",
    "    denominator_S_prime[denominator_S_prime == 0] = 1\n",
    "    S_prime = S_prime / denominator_S_prime\n",
    "    denominator_R_prime = R_prime.sum(axis=0)\n",
    "    denominator_R_prime[denominator_R_prime == 0] = 1\n",
    "    R_prime = R_prime / denominator_R_prime\n",
    "    #these are the vectors which contain the score of similarity\n",
    "    #and relevance\n",
    "    s = np.zeros(shape=(len(hub_nodes), 1), dtype=np.float)\n",
    "    r = np.zeros(shape=(len(authority_nodes), 1), dtype=np.float)\n",
    "\n",
    "    #at the beginning of the procedure we put the similarity\n",
    "    #of the user we want to give the recommendation equal to 1\n",
    "    index_of_node_to_recommend = np.where(circle_of_trust == node_id)[0][0]\n",
    "    s[index_of_node_to_recommend] = 1.\n",
    "\n",
    "    #init damping vector\n",
    "    alpha = 1 - damping_factor\n",
    "    alpha_vector = np.zeros(shape=(len(hub_nodes), 1), dtype=np.float)\n",
    "    alpha_vector[index_of_node_to_recommend] = alpha\n",
    "\n",
    "    #4th phase: run the algorithm\n",
    "    convergence = False\n",
    "    while not convergence:\n",
    "        s_ = s.copy()\n",
    "        r_ = r.copy()\n",
    "        r_ = S_prime.dot(s)\n",
    "        s_ = alpha_vector + (1 - alpha)*(R_prime.dot(r))\n",
    "        #compute difference and check if convergence has been reached\n",
    "        diff = abs(s_ - s)\n",
    "        if np.linalg.norm(diff) < tol:\n",
    "            convergence=True\n",
    "        #update real vectors\n",
    "        s = s_\n",
    "        r = r_\n",
    "\n",
    "    #5th phase: order by score and delete neighbors of node to be recommended\n",
    "    #of course we don't want to recommend people that the user already follow\n",
    "    neighbors_to_not_recommend = nx.neighbors(network, node_id)\n",
    "    relevance_scores = r.flatten()\n",
    "    if k_for_recommendation == -1:\n",
    "        k_for_recommendation = 0 #Take all the nodes!\n",
    "\n",
    "    neighbors_to_not_recommend = set(neighbors_to_not_recommend)\n",
    "    results = []\n",
    "    for node in relevance_scores.argsort()[::-1]:\n",
    "        if node not in neighbors_to_not_recommend and node != node_id:\n",
    "            results.append(((node_id, node))) # , relevance_scores[node]\n",
    "            if len(results) == k_for_recommendation:\n",
    "                break\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topj_bc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    bc_full = g.betweenness(directed=True)\n",
    "    g.vs['bc'] = bc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        # maj_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_bc_temp = min_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        min_bc_temp.sort()\n",
    "        topj = min_bc_temp[-j:]\n",
    "\n",
    "        inds = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "\n",
    "    if mode == 'maj':\n",
    "        maj_bc = maj_nodes_ig.get_attribute_values('bc')\n",
    "        maj_bc_temp = maj_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        maj_bc_temp.sort()\n",
    "        topj = maj_bc_temp[-j:]\n",
    "\n",
    "        inds = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "    \n",
    "    elif mode == 'both':\n",
    "        min_bc = min_nodes_ig.get_attribute_values('bc')\n",
    "        min_bc_temp = min_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        min_bc_temp.sort()\n",
    "        topj = min_bc_temp[-j:]\n",
    "\n",
    "        inds_min = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        maj_bc = maj_nodes_ig.get_attribute_values('bc')\n",
    "        maj_bc_temp = maj_nodes_ig.get_attribute_values('bc')\n",
    "\n",
    "        maj_bc_temp.sort()\n",
    "        topj = maj_bc_temp[-j:]\n",
    "\n",
    "        inds_maj = [bc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds_maj, inds_min\n",
    "\n",
    "\n",
    "def get_topj_evc_nodes(network_obj, j=5, to_ig=True, mode='min'):\n",
    "\n",
    "    def nx_to_ig(G):\n",
    "        return ig.Graph.from_networkx(G)\n",
    "\n",
    "    if to_ig:\n",
    "        g = nx_to_ig(network_obj)\n",
    "    else:\n",
    "        g = network_obj\n",
    "\n",
    "    min_nodes_ig = g.vs.select(m=1)\n",
    "    maj_nodes_ig = g.vs.select(m=0)\n",
    "\n",
    "    evc_full = g.eigenvector_centrality()\n",
    "    g.vs['evc'] = evc_full\n",
    "\n",
    "    if mode == 'min':\n",
    "        min_evc = min_nodes_ig.get_attribute_values('evc')\n",
    "        min_evc_temp = min_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        min_evc_temp.sort()\n",
    "        topj = min_evc_temp[-j:]\n",
    "\n",
    "        inds = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "    \n",
    "    elif mode == 'maj':\n",
    "        maj_evc = maj_nodes_ig.get_attribute_values('evc')\n",
    "        maj_evc_temp = maj_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        maj_evc_temp.sort()\n",
    "        topj = maj_evc_temp[-j:]\n",
    "\n",
    "        inds = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds\n",
    "\n",
    "    elif mode == 'both':\n",
    "        min_evc = min_nodes_ig.get_attribute_values('evc')\n",
    "        min_evc_temp = min_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        min_evc_temp.sort()\n",
    "        topj = min_evc_temp[-j:]\n",
    "\n",
    "        inds_min = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        maj_evc = maj_nodes_ig.get_attribute_values('evc')\n",
    "        maj_evc_temp = maj_nodes_ig.get_attribute_values('evc')\n",
    "\n",
    "        maj_evc_temp.sort()\n",
    "        topj = maj_evc_temp[-j:]\n",
    "\n",
    "        inds_maj = [evc_full.index(t) for t in topj][-j:]\n",
    "\n",
    "        return inds_maj, inds_min\n",
    "\n",
    "\n",
    "def _ppr(node_index, A, p, top):\n",
    "    pp = np.zeros(A.shape[0])\n",
    "    pp[node_index] = A.shape[0]\n",
    "    pr = pagerank_power(A, p=p, personalize=pp)\n",
    "    pr = pr.argsort()[-top-1:][::-1]\n",
    "    #time.sleep(0.01)\n",
    "    return list(pr[pr!=node_index][:top])\n",
    "\n",
    "def _get_circle_of_trust_per_node(A, p=0.85, top=10, num_cores=-1):\n",
    "    return Parallel(n_jobs=num_cores)(delayed(_ppr)(node_index, A, p, top) for node_index in np.arange(A.shape[0]))\n",
    "\n",
    "def _salsa_intervention(node_index, cot, A, top=10, to_add=[], network_obj=None):\n",
    "\n",
    "    BG = nx.Graph()\n",
    "    BG.add_nodes_from(['h{}'.format(vi) for vi in cot], bipartite=0)  # hubs\n",
    "    edges = [('h{}'.format(vi), int(vj)) for vi in cot for vj in np.argwhere(A[vi,:] != 0 )[:,1]]\n",
    "    BG.add_nodes_from(set([e[1] for e in edges]), bipartite=1)  # authorities\n",
    "    BG.add_edges_from(edges)\n",
    "\n",
    "    auths = [node for node, data in BG.nodes(data=True) if data['bipartite'] == 1]\n",
    "    hubs = [int(node[1:]) for node, data in BG.nodes(data=True) if data['bipartite'] == 0]\n",
    "\n",
    "    if node_index >= 8000:\n",
    "    #     to_add = to_add[1]\n",
    "    # else:\n",
    "    #     to_add = to_add[0]\n",
    "\n",
    "        new_auths = [auth_node for auth_node in to_add if auth_node not in auths]\n",
    "        new_auths_not_hubs = [n for n in new_auths if n not in hubs]\n",
    "\n",
    "        intervention_edges = [(hub, new_auth) for hub in random.sample(hubs, int(len(hubs) * 0.5)) for new_auth in new_auths_not_hubs]\n",
    "        BG.add_nodes_from(new_auths_not_hubs, bipartite=1)\n",
    "        BG.add_edges_from(intervention_edges)\n",
    "\n",
    "    centrality = Counter({n: c for n, c in nx.eigenvector_centrality_numpy(BG).items() if type(n) == int\n",
    "                                                                                        and n not in cot\n",
    "                                                                                        and n != node_index\n",
    "                                                                                        and n not in np.argwhere(A[node_index,:] != 0 )[:,1] })\n",
    "\n",
    "    return np.asarray([n for n, pev in centrality.most_common(top)])[:top]\n",
    "\n",
    "def get_intervention_recommendations(A, to_add=[], p=0.85, top=10, num_cores=-1, network_obj=None):\n",
    "    \n",
    "    cot_per_node = _get_circle_of_trust_per_node(A, p, top, num_cores)\n",
    "    results = Parallel(n_jobs=num_cores)(delayed(_salsa_intervention)(node_index, cot, A, top, to_add, network_obj) for node_index, cot in enumerate(cot_per_node))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "\n",
    "    def __init__(self, network, class_name='m', min_label=1, maj_label=0):\n",
    "        self.network = network\n",
    "        self.min_label, self.maj_label = min_label, maj_label\n",
    "        # self.min_nodes, self.maj_nodes = self.get_nodes_per_class()\n",
    "        # self.post_network = None\n",
    "        self.class_name = class_name\n",
    "        self.metrics_dict = {}\n",
    "    \n",
    "    def get_nodes_per_class(self, network):\n",
    "        \n",
    "        # class_\n",
    "        if self.class_name == 'm':\n",
    "            min_nodes_ig = network.vs.select(m=self.min_label)\n",
    "            maj_nodes_ig = network.vs.select(m=self.maj_label)\n",
    "\n",
    "        elif self.class_name == 'class_':\n",
    "            min_nodes_ig = network.vs.select(class_=self.min_label)\n",
    "            maj_nodes_ig = network.vs.select(class_=self.maj_label)\n",
    "        \n",
    "        elif self.class_name == 'age':\n",
    "            min_nodes_ig = network.vs.select(age=self.min_label)\n",
    "            maj_nodes_ig = network.vs.select(age=self.maj_label)\n",
    "\n",
    "        return min_nodes_ig, maj_nodes_ig\n",
    "    \n",
    "    def get_group_betweenness(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        bc_full = network.betweenness(directed=True)\n",
    "        network.vs['bc'] = bc_full\n",
    "\n",
    "        min_bc = np.average(np.array(min_nodes.get_attribute_values('bc')))\n",
    "        maj_bc = np.average(np.array(maj_nodes.get_attribute_values('bc')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['bc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['bc']['pre'] = {'min': min_bc, 'maj': maj_bc}\n",
    "        else:\n",
    "            self.metrics_dict['bc']['post'] = {'min': min_bc, 'maj': maj_bc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority BC: {min_bc}\")\n",
    "            print(f\"Majority BC: {maj_bc}\")\n",
    "\n",
    "        return min_bc, maj_bc\n",
    "    \n",
    "    def get_group_closeness(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        cc_full = network.closeness()\n",
    "        network.vs['cc'] = cc_full\n",
    "\n",
    "        min_cc = np.average(np.array(min_nodes.get_attribute_values('cc')))\n",
    "        maj_cc = np.average(np.array(maj_nodes.get_attribute_values('cc')))\n",
    "\n",
    "        if is_pre:\n",
    "            self.metrics_dict['cc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['cc']['pre'] = {'min': min_cc, 'maj': maj_cc}\n",
    "        else:\n",
    "            self.metrics_dict['cc']['post'] = {'min': min_cc, 'maj': maj_cc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority CC: {min_cc}\")\n",
    "            print(f\"Majority CC: {maj_cc}\")\n",
    "\n",
    "        return min_cc, maj_cc\n",
    "    \n",
    "    def get_group_eigenvector_centrality(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        evc_full = network.eigenvector_centrality()\n",
    "        network.vs['evc'] = evc_full\n",
    "\n",
    "        min_evc = np.average(np.array(min_nodes.get_attribute_values('evc')))\n",
    "        maj_evc = np.average(np.array(maj_nodes.get_attribute_values('evc')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['evc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['evc']['pre'] = {'min': min_evc, 'maj': maj_evc}\n",
    "        else:\n",
    "            self.metrics_dict['evc']['post'] = {'min': min_evc, 'maj': maj_evc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority EVC: {min_evc} / normalized: {min_evc/(min_evc+maj_evc)}\")\n",
    "            print(f\"Majority EVC: {maj_evc} / normalized: {maj_evc/(min_evc+maj_evc)}\")\n",
    "\n",
    "        return min_evc, maj_evc\n",
    "    \n",
    "    def get_group_degree_centrality(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "\n",
    "        dc_full = network.vs.indegree()\n",
    "        network.vs['dc'] = dc_full\n",
    "\n",
    "        min_dc = np.average(np.array(min_nodes.get_attribute_values('dc')))\n",
    "        maj_dc = np.average(np.array(maj_nodes.get_attribute_values('dc')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['dc'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['dc']['pre'] = {'min': min_dc, 'maj': maj_dc}\n",
    "        else:\n",
    "            self.metrics_dict['dc']['post'] = {'min': min_dc, 'maj': maj_dc}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority DC: {min_dc}\")\n",
    "            print(f\"Majority DC: {maj_dc}\")\n",
    "\n",
    "        return min_dc, maj_dc\n",
    "\n",
    "    def get_group_pagerank(self, network, is_pre=True, verbose=True):\n",
    "\n",
    "        min_nodes, maj_nodes = self.get_nodes_per_class(network)\n",
    "        pr_full = network.pagerank()\n",
    "\n",
    "        network.vs['pr'] = pr_full\n",
    "\n",
    "        min_pr = np.average(np.array(min_nodes.get_attribute_values('pr')))\n",
    "        maj_pr = np.average(np.array(maj_nodes.get_attribute_values('pr')))\n",
    "        \n",
    "        if is_pre:\n",
    "            self.metrics_dict['pr'] = {'pre': {}, 'post': {}}\n",
    "            self.metrics_dict['pr']['pre'] = {'min': min_pr, 'maj': maj_pr}\n",
    "        else:\n",
    "            self.metrics_dict['pr']['post'] = {'min': min_pr, 'maj': maj_pr}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Minority PR: {min_pr}\")\n",
    "            print(f\"Majority PR: {maj_pr}\")\n",
    "\n",
    "        return min_pr, maj_pr\n",
    "    \n",
    "    def remove_zero_degree(self, verbose=False):\n",
    "        self.to_remove = [node for node, degree in dict(self.network.degree()).items() if degree < 1]\n",
    "        self.network.remove_nodes_from(self.to_remove)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Removed {len(self.to_remove)} nodes with degree zero.\")\n",
    "        return self.to_remove\n",
    "\n",
    "    def add_nodes(self):\n",
    "        self.network.add_nodes_from(self.to_remove)\n",
    "        # print(f\"Added {len(self.to_remove)} nodes.\")\n",
    "\n",
    "    def sample_and_delete_edges(self, num_to_sample=100, remove_edges=True):\n",
    "        edge_list = list(self.network.edges())\n",
    "\n",
    "        self.random_edges_to_delete = random.sample(edge_list, num_to_sample)\n",
    "        nodes_with_del_out_edge = [tup[0] for tup in self.random_edges_to_delete]\n",
    "        self.unique_nodes = list(set(nodes_with_del_out_edge))\n",
    "\n",
    "        self.nodes_with_edge_count = Counter(node for node in nodes_with_del_out_edge)\n",
    "\n",
    "        if remove_edges:\n",
    "            self.network.remove_edges_from(self.random_edges_to_delete)\n",
    "\n",
    "            # print(f'# nodes with deleted out edge {len(nodes_with_del_out_edge)}')\n",
    "            print(f'# unique nodes with deleted out edge: {len(self.unique_nodes)}')\n",
    "        else:\n",
    "            print(f'# unique nodes with sampled out edge: {len(self.unique_nodes)} - No edges where deleted.')\n",
    "\n",
    "        return self.random_edges_to_delete, self.unique_nodes, self.nodes_with_edge_count\n",
    "\n",
    "    def reconstruct_edges(self, strategy='bulk', k_for_circle_of_trust=20, salsa_new=True, run=0, wtf=None, to_pick=1,\n",
    "                            consider_all_nodes=True, num_to_consider=1000, add_top_k=False, delete_edges=True, \n",
    "                            delete_first=False, network_file='', network_obj=None, class_name='m',\n",
    "                            intervene=False, to_add=None, network=None, strat='bc', metric_mode='min', j=5, target_mode='min'):\n",
    "\n",
    "\n",
    "        if strategy == 'intervention':\n",
    "            pass\n",
    "\n",
    "\n",
    "        if strategy == 'new':\n",
    "\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            self.top_k_frac_min = []\n",
    "            self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "\n",
    "\n",
    "            top = 10\n",
    "            p = 0.85\n",
    "\n",
    "            sample = random.sample(self.network.nodes(), num_to_consider)\n",
    "\n",
    "\n",
    "            for k in range(to_pick):\n",
    "                \n",
    "                A = nx.adjacency_matrix(self.network)\n",
    "\n",
    "                cots = Parallel(n_jobs=-1)(delayed(_ppr)(node, A, p, top) for node in sample)\n",
    "                z = dict(zip(sample, cots))\n",
    "                recommendations = Parallel(n_jobs=-1)(delayed(_salsa2)(node, z[node], A, ix=None, top=10) for node in z.keys())\n",
    "\n",
    "                for arr in recommendations:\n",
    "\n",
    "                    if network_file == '':\n",
    "                            network = network_obj\n",
    "                    else:\n",
    "                            network = read_pkl_graph(network_file)\n",
    "                    \n",
    "                    top_k_classes = [network.nodes()[node][class_name] for node in arr]\n",
    "                    frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                    frac_maj = 1-frac_min\n",
    "                    self.top_k_frac_min.append(frac_min*100)\n",
    "\n",
    "                zz = dict(zip(sample, recommendations))\n",
    "                for key in zz.keys():\n",
    "                    to_add = zz[key][0]\n",
    "                    self.network.add_edge(key, to_add)\n",
    "                    self.salsa_all_edges.append((key, to_add))\n",
    "                    self.network.remove_edges_from(random.sample(list(self.network.out_edges(key)), 1))    \n",
    "\n",
    "\n",
    "        if strategy == 'consecutively':\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            self.top_k_frac_min = []\n",
    "            self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "            num_edges_added = 0\n",
    "\n",
    "            if salsa_new:\n",
    "\n",
    "                wtf = wtf\n",
    "\n",
    "                if consider_all_nodes:\n",
    "                \n",
    "                    for node in self.network.nodes():\n",
    "                        # num_to_add = self.nodes_with_edge_count[node]\n",
    "\n",
    "                        wtf_node_i = list(wtf[node])\n",
    "                        if len(wtf_node_i) >= to_pick:\n",
    "                            self.salsa_edges_per_node[node] = []\n",
    "\n",
    "                            if add_top_k:\n",
    "                                pick = wtf_node_i[0:to_pick]\n",
    "                                \n",
    "                            else:\n",
    "                                pick = random.sample(wtf_node_i, to_pick)\n",
    "\n",
    "                            \n",
    "                            if delete_edges:\n",
    "                                out_edges_node = list(self.network.out_edges(node))\n",
    "                               \n",
    "                                if len(out_edges_node) >= to_pick:\n",
    "                                    self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                                    for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                    self.salsa_edges_per_node[node].append(pick)\n",
    "\n",
    "                                    top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                    frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                    frac_maj = 1-frac_min\n",
    "                                    self.top_k_frac_min.append(frac_min*100)                                \n",
    "                                    \n",
    "                            else:\n",
    "                                for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                self.salsa_edges_per_node[node].append(pick)\n",
    "\n",
    "                                top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                frac_maj = 1-frac_min\n",
    "                                self.top_k_frac_min.append(frac_min*100)                                \n",
    "                            \n",
    "                                num_edges_added += 1\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    all_nodes = list(self.network.nodes())\n",
    "                    sample = random.sample(all_nodes, num_to_consider)\n",
    "\n",
    "                    for node in sample:\n",
    "\n",
    "                        wtf_node_i = list(wtf[node])\n",
    "\n",
    "\n",
    "                        if len(wtf_node_i) >= to_pick:\n",
    "                            self.salsa_edges_per_node[node] = []\n",
    "\n",
    "                            if add_top_k:\n",
    "                                pick = wtf_node_i[0:to_pick]\n",
    "                            else:\n",
    "                                pick = random.sample(wtf_node_i, to_pick)\n",
    "\n",
    "                            if delete_edges:\n",
    "\n",
    "                                out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                                if len(out_edges_node) >= to_pick:\n",
    "                                    self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                                    for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                    self.salsa_edges_per_node[node].append(pick)   \n",
    "\n",
    "                                    top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                    frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                    frac_maj = 1-frac_min\n",
    "                                    self.top_k_frac_min.append(frac_min*100)                                \n",
    "\n",
    "                            else:\n",
    "\n",
    "                                for new_node in pick:\n",
    "                                        self.network.add_edge(node, new_node)\n",
    "                                        self.salsa_all_edges.append((node, new_node))\n",
    "                                self.salsa_edges_per_node[node].append(pick)   \n",
    "\n",
    "                                top_k_classes = [self.network.nodes()[node][self.class_name] for node in wtf_node_i]\n",
    "                                frac_min = sum(top_k_classes) / len(top_k_classes)\n",
    "                                frac_maj = 1-frac_min\n",
    "                                self.top_k_frac_min.append(frac_min*100) \n",
    "                                \n",
    "            \n",
    "            else: \n",
    "\n",
    "                for node in self.unique_nodes:\n",
    "                    self.salsa_edges_per_node[node] = []\n",
    "                    for k in range(self.nodes_with_edge_count[node]):\n",
    "\n",
    "                        top_all = twitter_wtf(network=self.network, sparse_adj=nx.adjacency_matrix(self.network), node_id=node, k_for_recommendation=-1)\n",
    "                        top_1 = top_all[0]\n",
    "                        self.network.add_edge(node, top_1[1])\n",
    "                        \n",
    "                        top_10pct = top_all[:int(len(top_all)*0.1)]\n",
    "                        \n",
    "                        # top_10pct_classes = [self.network.nodes()[edge[1]][self.class_name] for edge in top_10pct]\n",
    "                        top_10pct_classes = [self.network.nodes()[edge[-1]][self.class_name] for edge in top_10pct]\n",
    "                        frac_min = sum(top_10pct_classes) / len(top_10pct_classes)\n",
    "                        frac_maj = 1-frac_min\n",
    "                        self.top_10pct_frac_min.append(frac_min)\n",
    "\n",
    "                        self.salsa_edges_per_node[node].append(top_1)\n",
    "                        self.salsa_all_edges.append(top_1)\n",
    "                        \n",
    "                        num_edges_added += 1   \n",
    "        \n",
    "        elif strategy == 'random':\n",
    "\n",
    "            if consider_all_nodes:\n",
    "\n",
    "                self.salsa_edges_per_node = {}\n",
    "                self.salsa_all_edges = []\n",
    "                self.top_k_frac_min = []\n",
    "                self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "                \n",
    "                self.all_nodes = list(self.network.nodes())\n",
    "                for node in self.all_nodes:\n",
    "                    \n",
    "                    pick = random.sample(self.all_nodes, to_pick)\n",
    "\n",
    "                    if delete_edges:\n",
    "                        out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                        if len(out_edges_node) >= to_pick:\n",
    "                            self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                            for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                            self.salsa_edges_per_node[node] = pick\n",
    "                    else:\n",
    "                        for new_node in pick:\n",
    "                            self.network.add_edge(node, new_node)\n",
    "                            self.salsa_all_edges.append((node, new_node))\n",
    "                        self.salsa_edges_per_node[node] = pick\n",
    "            \n",
    "            else:\n",
    "                self.salsa_edges_per_node = {}\n",
    "                self.salsa_all_edges = []\n",
    "                self.top_k_frac_min = []\n",
    "                self.edge_type = {'mm': 0,\n",
    "                            'MM': 0,\n",
    "                            'mM': 0,\n",
    "                            'Mm': 0}\n",
    "                \n",
    "                self.all_nodes = list(self.network.nodes())\n",
    "                k = random.sample(self.all_nodes, num_to_consider)\n",
    "\n",
    "                for node in k:\n",
    "                    \n",
    "                    pick = random.sample(k, to_pick)\n",
    "\n",
    "                    if delete_edges:\n",
    "                        out_edges_node = list(self.network.out_edges(node))\n",
    "\n",
    "                        if len(out_edges_node) >= to_pick:\n",
    "                            self.network.remove_edges_from(random.sample(out_edges_node, to_pick))\n",
    "\n",
    "                            for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                            self.salsa_edges_per_node[node] = pick\n",
    "\n",
    "                    else:\n",
    "                        for new_node in pick:\n",
    "                                self.network.add_edge(node, new_node)\n",
    "                                self.salsa_all_edges.append((node, new_node))\n",
    "                        self.salsa_edges_per_node[node] = pick\n",
    "\n",
    "        self.avg_frac_min = 0\n",
    "\n",
    "        if strategy != 'random':\n",
    "            self.avg_frac_min, self.sd_frac_min = np.round(np.average(self.top_k_frac_min), 5), np.round(np.std(self.top_k_frac_min), 5)\n",
    "\n",
    "\n",
    "        return self.salsa_edges_per_node, self.salsa_all_edges, self.edge_type, self.avg_frac_min\n",
    "\n",
    "    def add_edges(self, strategy='bulk', k_for_circle_of_trust=20):\n",
    "\n",
    "        if strategy == 'bulk':\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            num_edges_added = 0\n",
    "            for node in self.unique_nodes:\n",
    "\n",
    "                recommended_nodes = twitter_wtf(network=self.network, sparse_adj=nx.adjacency_matrix(self.network), node_id=node, k_for_circle_of_trust=k_for_circle_of_trust, k_for_recommendation=self.nodes_with_edge_count[node])\n",
    "                self.network.add_edges_from(recommended_nodes)\n",
    "                self.salsa_edges_per_node[node] = recommended_nodes\n",
    "                self.salsa_all_edges.append(recommended_nodes[0])\n",
    "                num_edges_added += len(recommended_nodes)\n",
    "\n",
    "        elif strategy == 'random':\n",
    "            self.salsa_edges_per_node = {}\n",
    "            self.salsa_all_edges = []\n",
    "            num_edges_added = 0\n",
    "            self.all_nodes = list(self.network.nodes())\n",
    "            for node in self.unique_nodes:\n",
    "                \n",
    "                for z in range(self.nodes_with_edge_count[node]):\n",
    "                    pick = random.sample(self.all_nodes, 1)[0]\n",
    "\n",
    "                    self.network.add_edge(node, pick)\n",
    "                    self.salsa_edges_per_node[node] = pick\n",
    "                    self.salsa_all_edges.append(pick)\n",
    "                    num_edges_added += 1\n",
    "        \n",
    "        return self.salsa_edges_per_node, self.salsa_all_edges\n",
    "\n",
    "    def get_relative_metric_changes(self, metric='all', round=5):\n",
    "\n",
    "        if metric == 'all':\n",
    "           \n",
    "            bc_vals = self.metrics_dict['bc']\n",
    "            print(\"Relative BC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((bc_vals['post']['maj']/bc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((bc_vals['post']['min']/bc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "        \n",
    "            cc_vals = self.metrics_dict['cc']\n",
    "            print(\"Relative CC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((cc_vals['post']['maj']/cc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((cc_vals['post']['min']/cc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "        \n",
    "            evc_vals = self.metrics_dict['evc']\n",
    "            print(\"Relative EVC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((evc_vals['post']['maj']/evc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            # print(f\"Normalized Majority change (pre->post) %: {np.round(((evc_vals['post']['maj']/(evc_vals['post']['maj']+evc_vals['post']['min']))/(evc_vals['pre']['maj']/(evc_vals['pre']['maj']+evc_vals['pre']['min']))-1)*100)}%\")\n",
    "            # print(f\"Normalized Minority change (pre->post) %: {np.round((evc_vals['post']['min']/(evc_vals['pre']['min']+evc_vals['pre']['maj'])-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "\n",
    "            dc_vals = self.metrics_dict['dc']\n",
    "            print(\"Relative DC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((dc_vals['post']['maj']/dc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((dc_vals['post']['min']/dc_vals['pre']['min']-1)*100, round)}%\")\n",
    "            print(\"\")\n",
    "\n",
    "        elif metric == 'bc':\n",
    "            bc_vals = self.metrics_dict['bc']\n",
    "\n",
    "            print(\"Relative BC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((bc_vals['post']['maj']/bc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((bc_vals['post']['min']/bc_vals['pre']['min']-1)*100, round)}%\")\n",
    "        \n",
    "        elif metric == 'cc':\n",
    "            cc_vals = self.metrics_dict['cc']\n",
    "            \n",
    "            print(\"Relative CC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((cc_vals['post']['maj']/cc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((cc_vals['post']['min']/cc_vals['pre']['min']-1)*100, round)}%\")\n",
    "        \n",
    "        elif metric == 'evc':\n",
    "            evc_vals = self.metrics_dict['evc']\n",
    "            \n",
    "            print(\"Relative EVC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((evc_vals['post']['maj']/evc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round)}%\")\n",
    "        \n",
    "        elif metric == 'dc':\n",
    "            dc_vals = self.metrics_dict['dc']\n",
    "            \n",
    "            print(\"Relative DC changes\")\n",
    "            print(\"---------------------------\")\n",
    "            print(f\"Majority change (pre->post) %: {np.round((dc_vals['post']['maj']/dc_vals['pre']['maj']-1)*100, round)}%\")\n",
    "            print(f\"Minority change (pre->post) %: {np.round((dc_vals['post']['min']/dc_vals['pre']['min']-1)*100, round)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_to_ig(G):\n",
    "    return ig.Graph.from_networkx(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_class_attribute(G, from_='', to_=''):\n",
    "    class_ = G.vs()[from_]\n",
    "    G.vs[to_] = class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_degree(network, verbose=False):\n",
    "    to_remove = [node for node, degree in dict(network.degree()).items() if degree < 1]\n",
    "    network.remove_nodes_from(to_remove)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Removed {len(to_remove)} nodes with degree zero.\")\n",
    "    return to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_n_simulations(network_obj=None, network_file='', n=5, num_to_sample=10000, delete_edges=True, verbose=False, round=5, \n",
    "                      remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                      strategy='bulk', save_results=True, save_as='', to_txt=True, salsa_new=True, quick_test=False,\n",
    "                      to_pick=1, consider_all_nodes=True, num_to_consider=1000, add_top_k=False,\n",
    "                      intervene=False, strat='evc', metric_mode='both', j=5, target_mode='min', recomms=None):\n",
    "\n",
    "    \n",
    "\n",
    "    changes_dict_min = {'bc': [],\n",
    "                        'cc': [],\n",
    "                        'evc_norm': [],\n",
    "                        'evc': [],\n",
    "                        'evc_abs': [],\n",
    "                        'dc': [],\n",
    "                        'pr': [],\n",
    "                        'c_coeff': [],\n",
    "                        'aspl': []\n",
    "                        }\n",
    "\n",
    "    changes_dict_maj = {'bc': [],\n",
    "                        'cc': [],\n",
    "                        'evc_norm': [],\n",
    "                        'evc': [],\n",
    "                        'evc_abs': [],\n",
    "                        'dc': [],\n",
    "                        'pr': [],\n",
    "                        'c_coeff': [],\n",
    "                        'aspl': []\n",
    "                        }\n",
    "\n",
    "    recommended_nodes_dict = {}\n",
    "\n",
    "    edge_type_dict = {}\n",
    "\n",
    "    frac_mins_list = []\n",
    "\n",
    "    # overlaps = []\n",
    "    \n",
    "    wtf = None\n",
    "\n",
    "    if salsa_new and not intervene:\n",
    "        wtf = who_to_follow_rank(A=nx.adjacency_matrix(network_obj), njobs=-1, intervene=intervene, network=network_obj, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode)\n",
    "    \n",
    "    elif intervene:\n",
    "        wtf = recomms\n",
    "        \n",
    "    \n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        if network_file == '':\n",
    "            g = network_obj\n",
    "        else:\n",
    "            g = read_pkl_graph(network_file)\n",
    "       \n",
    "        g = reindex_nodes(g, name_as_str=False)\n",
    "\n",
    "        a = Analyzer(network=g, class_name=class_name, min_label=min_label, maj_label=maj_label)\n",
    "\n",
    "        if remove_zero_degree_nodes:\n",
    "            _ = a.remove_zero_degree()\n",
    "\n",
    "        if not quick_test:\n",
    "            if i == 0:\n",
    "                min_c_coeff, maj_c_coeff = get_avg_clustering_coefficient(network_obj=a.network, mode='zero', class_name=class_name, group_names=[min_label, maj_label])\n",
    "                min_aspl, maj_aspl = get_avg_sp_lengths(network_obj=a.network, mode='out', class_name=class_name, group_names=[min_label, maj_label])\n",
    "\n",
    "            g_pre = nx_to_ig(a.network)\n",
    "\n",
    "            \n",
    "            _ = a.get_group_betweenness(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_closeness(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_eigenvector_centrality(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_degree_centrality(g_pre, is_pre=True, verbose=verbose)\n",
    "            _ = a.get_group_pagerank(g_pre, is_pre=True, verbose=verbose)\n",
    "\n",
    "        if remove_zero_degree_nodes:\n",
    "            if len(a.to_remove) > 0:\n",
    "                a.add_nodes()\n",
    "        \n",
    "        # if not salsa_new:\n",
    "        #     _ = a.sample_and_delete_edges(num_to_sample=num_to_sample, remove_edges=delete_edges)\n",
    "\n",
    "\n",
    "        \n",
    "        salsa_edges_per_node, salsa_all_edges, edge_type, avg_frac_min = a.reconstruct_edges(strategy=strategy, \n",
    "                                            salsa_new=salsa_new, run=i, wtf=wtf, to_pick=to_pick, consider_all_nodes=consider_all_nodes, \n",
    "                                            num_to_consider=num_to_consider, add_top_k=add_top_k, delete_edges=delete_edges,\n",
    "                                            network_file=network_file, network_obj=network_obj, class_name=class_name,\n",
    "                                            intervene=intervene, network=network_obj, strat=strat, metric_mode=metric_mode, j=j, target_mode=target_mode)\n",
    "        \n",
    "\n",
    "        edge_type_dict[i] = edge_type\n",
    "        frac_mins_list.append(avg_frac_min)\n",
    "        \n",
    "        if remove_zero_degree_nodes:\n",
    "            _ = a.remove_zero_degree()\n",
    "\n",
    "        min_ccoeff_post, maj_ccoeff_post = get_avg_clustering_coefficient(network_obj=a.network, mode='zero', class_name=class_name, group_names=[min_label, maj_label])\n",
    "        min_aspl_post, maj_aspl_post = get_avg_sp_lengths(network_obj=a.network, mode='out', class_name=class_name, group_names=[min_label, maj_label])\n",
    "\n",
    "        changes_dict_maj['c_coeff'].append(np.round((maj_ccoeff_post/maj_c_coeff-1)*100, round))\n",
    "        changes_dict_min['c_coeff'].append(np.round((min_ccoeff_post/min_c_coeff-1)*100, round))\n",
    "\n",
    "        changes_dict_maj['aspl'].append(np.round((maj_aspl_post/maj_aspl-1)*100, round))\n",
    "        changes_dict_min['aspl'].append(np.round((min_aspl_post/min_aspl-1)*100, round))\n",
    "\n",
    "        g_post = nx_to_ig(a.network)\n",
    "\n",
    "        _ = a.get_group_betweenness(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_closeness(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_eigenvector_centrality(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_degree_centrality(g_post, is_pre=False, verbose=verbose)\n",
    "        _ = a.get_group_pagerank(g_post, is_pre=False, verbose=verbose)\n",
    "        \n",
    "        bc_vals = a.metrics_dict['bc']\n",
    "        changes_dict_maj['bc'].append(np.round((bc_vals['post']['maj']/bc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['bc'].append(np.round((bc_vals['post']['min']/bc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        cc_vals = a.metrics_dict['cc']\n",
    "        changes_dict_maj['cc'].append(np.round((cc_vals['post']['maj']/cc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['cc'].append(np.round((cc_vals['post']['min']/cc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        evc_vals = a.metrics_dict['evc']\n",
    "        norm_evc_min_pre = evc_vals['pre']['min'] / (evc_vals['pre']['min'] + evc_vals['pre']['maj'])\n",
    "        norm_evc_maj_pre = evc_vals['pre']['maj'] / (evc_vals['pre']['min'] + evc_vals['pre']['maj'])\n",
    "\n",
    "        norm_evc_min_post = evc_vals['post']['min'] / (evc_vals['post']['min'] + evc_vals['post']['maj'])\n",
    "        norm_evc_maj_post = evc_vals['post']['maj'] / (evc_vals['post']['min'] + evc_vals['post']['maj'])\n",
    "\n",
    "        changes_dict_maj['evc_norm'].append(np.round((norm_evc_maj_post/norm_evc_maj_pre-1)*100, round))\n",
    "        changes_dict_min['evc_norm'].append(np.round((norm_evc_min_post/norm_evc_min_pre-1)*100, round))\n",
    "\n",
    "        changes_dict_maj['evc'].append(np.round((evc_vals['post']['maj']/evc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['evc'].append(np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        # changes_dict_maj['evc_abs'].append(np.round((, round))\n",
    "        # changes_dict_min['evc_abs'].append(np.round((evc_vals['post']['min']/evc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        dc_vals = a.metrics_dict['dc']\n",
    "        changes_dict_maj['dc'].append(np.round((dc_vals['post']['maj']/dc_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['dc'].append(np.round((dc_vals['post']['min']/dc_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        pr_vals = a.metrics_dict['pr']\n",
    "        changes_dict_maj['pr'].append(np.round((pr_vals['post']['maj']/pr_vals['pre']['maj']-1)*100, round))\n",
    "        changes_dict_min['pr'].append(np.round((pr_vals['post']['min']/pr_vals['pre']['min']-1)*100, round))\n",
    "\n",
    "        recommended_nodes_dict[i] = salsa_all_edges\n",
    "\n",
    "        print(f'Finished run {i}')\n",
    "\n",
    "    # return changes_dict_maj, changes_dict_min\n",
    "    avg_bc_change_maj, sd_bc_maj = np.round(np.average(changes_dict_maj['bc']), round), np.round(np.std(changes_dict_maj['bc']), round)\n",
    "    avg_bc_change_min, sd_bc_min = np.round(np.average(changes_dict_min['bc']), round), np.round(np.std(changes_dict_min['bc']), round)\n",
    "\n",
    "    avg_cc_change_maj, sd_cc_maj = np.round(np.average(changes_dict_maj['cc']), round), np.round(np.std(changes_dict_maj['cc']), round)\n",
    "    avg_cc_change_min, sd_cc_min = np.round(np.average(changes_dict_min['cc']), round), np.round(np.std(changes_dict_min['cc']), round)\n",
    "\n",
    "    avg_evcn_change_maj, sd_evcn_maj = np.round(np.average(changes_dict_maj['evc_norm']), round), np.round(np.std(changes_dict_maj['evc_norm']), round)\n",
    "    avg_evcn_change_min, sd_evcn_min = np.round(np.average(changes_dict_min['evc_norm']), round), np.round(np.std(changes_dict_min['evc_norm']), round)\n",
    "\n",
    "    avg_evc_change_maj, sd_evc_maj = np.round(np.average(changes_dict_maj['evc']), round), np.round(np.std(changes_dict_maj['evc']), round)\n",
    "    avg_evc_change_min, sd_evc_min = np.round(np.average(changes_dict_min['evc']), round), np.round(np.std(changes_dict_min['evc']), round)\n",
    "\n",
    "    avg_dc_change_maj, sd_dc_maj = np.round(np.average(changes_dict_maj['dc']), round), np.round(np.std(changes_dict_maj['dc']), round)\n",
    "    avg_dc_change_min, sd_dc_min = np.round(np.average(changes_dict_min['dc']), round), np.round(np.std(changes_dict_min['dc']), round)\n",
    "\n",
    "    avg_pr_change_maj, sd_pr_maj = np.round(np.average(changes_dict_maj['pr']), round), np.round(np.std(changes_dict_maj['pr']), round)\n",
    "    avg_pr_change_min, sd_pr_min = np.round(np.average(changes_dict_min['pr']), round), np.round(np.std(changes_dict_min['pr']), round)\n",
    "\n",
    "    avg_ccoeff_change_maj, sd_ccoeff_maj = np.round(np.average(changes_dict_maj['c_coeff']), round), np.round(np.std(changes_dict_maj['c_coeff']), round)\n",
    "    avg_ccoeff_change_min, sd_ccoeff_min = np.round(np.average(changes_dict_min['c_coeff']), round), np.round(np.std(changes_dict_min['c_coeff']), round)\n",
    "\n",
    "    avg_aspl_change_maj, sd_aspl_maj = np.round(np.average(changes_dict_maj['aspl']), round), np.round(np.std(changes_dict_maj['aspl']), round)\n",
    "    avg_aspl_change_min, sd_aspl_min = np.round(np.average(changes_dict_min['aspl']), round), np.round(np.std(changes_dict_min['aspl']), round)\n",
    "\n",
    "    mm = []\n",
    "    mM = []\n",
    "    MM = []\n",
    "    Mm = []\n",
    "\n",
    "    if network_file == '':\n",
    "            network = network_obj\n",
    "    else:\n",
    "            network = read_pkl_graph(network_file)\n",
    "\n",
    "    for run in recommended_nodes_dict.keys():\n",
    "\n",
    "        edge_type = {'mm': 0,\n",
    "            'MM': 0,\n",
    "            'mM': 0,\n",
    "            'Mm': 0}\n",
    "\n",
    "        for edge in recommended_nodes_dict[run]:\n",
    "            class_k = network.nodes()[edge[0]][class_name]\n",
    "            class_j = network.nodes()[edge[-1]][class_name]\n",
    "            \n",
    "            if class_k == 1 and class_j == 1:\n",
    "                edge_type['mm'] += 1\n",
    "            elif class_k == 1 and class_j == 0:\n",
    "                edge_type['mM'] += 1\n",
    "            elif class_k == 0 and class_j == 1:\n",
    "                edge_type['Mm'] += 1\n",
    "            elif class_k == 0 and class_j == 0:\n",
    "                edge_type['MM'] += 1\n",
    "        \n",
    " \n",
    "        mm.append(edge_type['mm'] / len(recommended_nodes_dict[run]) * 100)\n",
    "        MM.append(edge_type['MM'] / len(recommended_nodes_dict[run]) * 100)\n",
    "        mM.append(edge_type['mM'] / len(recommended_nodes_dict[run]) * 100)\n",
    "        Mm.append(edge_type['Mm'] / len(recommended_nodes_dict[run]) * 100)\n",
    "\n",
    "    # for run in edge_type_dict.keys():\n",
    "    #     mm.append(edge_type_dict[run]['mm'])\n",
    "    #     mM.append(edge_type_dict[run]['mM'])\n",
    "    #     MM.append(edge_type_dict[run]['MM'])\n",
    "    #     Mm.append(edge_type_dict[run]['Mm'])\n",
    "\n",
    "    # mm = [e/len(salsa_all_edges)*100 for e in mm]\n",
    "    # mM = [e/len(salsa_all_edges)*100 for e in mM]\n",
    "    # MM = [e/len(salsa_all_edges)*100 for e in MM]\n",
    "    # Mm = [e/len(salsa_all_edges)*100 for e in Mm]\n",
    "\n",
    "    avg_mm, sd_mm = np.round(np.average(mm), round), np.round(np.std(mm), round)\n",
    "    avg_mM, sd_mM = np.round(np.average(mM), round), np.round(np.std(mM), round)\n",
    "    avg_MM, sd_MM = np.round(np.average(MM), round), np.round(np.std(MM), round)\n",
    "    avg_Mm, sd_Mm = np.round(np.average(Mm), round), np.round(np.std(Mm), round)\n",
    "\n",
    "    # recommendations\n",
    "    avg_avg_frac_min, sd_frac_min = [], []\n",
    "    if strategy != 'random':\n",
    "        avg_avg_frac_min, sd_frac_min = np.round(np.average(frac_mins_list), round), np.round(np.std(frac_mins_list), round)\n",
    "    \n",
    "    # additions\n",
    "    avg_recomm_share_min, recomm_sd_min, avg_recomm_share_maj, recomm_sd_maj = get_recommendation_group_ratios(network_file='', network_obj=network_obj, recommended_nodes=recommended_nodes_dict, class_name=class_name)\n",
    "\n",
    "    if to_txt:\n",
    "        save_as_txt = save_as[:-4] + '.txt'\n",
    "        f = open(save_as_txt, 'w')\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "        f.write(f\"Mean stats and standard deviation for {n} runs\\n\")\n",
    "        f.write(\"---------------------------------------------------\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Added Nodes Min/Maj Fraction\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f'Avg. share of maj: {avg_recomm_share_maj}% ({recomm_sd_maj})\\n')\n",
    "        f.write(f'Avg. share of min: {avg_recomm_share_min}% ({recomm_sd_min})\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Top 10 Minority Fraction Recommendations\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f'Avg. minority fraction among Top 10 Recommendations: {avg_avg_frac_min}% ({sd_frac_min})\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Edge types\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f'mm: {avg_mm}% ({sd_mm})\\n')\n",
    "        f.write(f'mM: {avg_mM}% ({sd_mM})\\n')\n",
    "        f.write(f'MM: {avg_MM}% ({sd_MM})\\n')\n",
    "        f.write(f'Mm: {avg_Mm}% ({sd_Mm})\\n')\n",
    "        f.write(\"\\n\")\n",
    "        f.write('DC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_dc_change_maj} ({sd_dc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_dc_change_min} ({sd_dc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('BC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_bc_change_maj} ({sd_bc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_bc_change_min} ({sd_bc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('CC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_cc_change_maj} ({sd_cc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_cc_change_min} ({sd_cc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('EVC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_evc_change_maj} ({sd_evc_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_evc_change_min} ({sd_evc_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Norm EVC Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_evcn_change_maj} ({sd_evcn_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_evcn_change_min} ({sd_evcn_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('PR Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_pr_change_maj} ({sd_pr_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_pr_change_min} ({sd_pr_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Clustering Coeff Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_ccoeff_change_maj} ({sd_ccoeff_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_ccoeff_change_min} ({sd_ccoeff_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write('Avg. Shortest Path Changes\\n')\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"Majority: {avg_aspl_change_maj} ({sd_aspl_maj})\\n\")\n",
    "        f.write(f\"Minority: {avg_aspl_change_min} ({sd_aspl_min})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "        # f.write(f\"Avg. overlap: {np.round(np.average(overlaps), round)}\\n\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"Mean stats and standard deviation for {n} runs\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    print('Added Nodes Min/Maj Fraction')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'Avg. share of maj: {avg_recomm_share_maj}% ({recomm_sd_maj})')\n",
    "    print(f'Avg. share of min: {avg_recomm_share_min}% ({recomm_sd_min})')\n",
    "    print(\"\")\n",
    "    print('Edge types')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'mm: {avg_mm}% ({sd_mm})')\n",
    "    print(f'mM: {avg_mM}% ({sd_mM})')\n",
    "    print(f'MM: {avg_MM}% ({sd_MM})')\n",
    "    print(f'Mm: {avg_Mm}% ({sd_Mm})')\n",
    "    print(\"\")\n",
    "    print('Top 10 Minority Fraction Recommendations')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'Avg. minority fraction: {avg_avg_frac_min}% ({sd_frac_min})')\n",
    "    print(\"\")\n",
    "    print('DC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_dc_change_maj} ({sd_dc_maj})\")\n",
    "    print(f\"Minority: {avg_dc_change_min} ({sd_dc_min})\")\n",
    "    print(\"\")\n",
    "    print('BC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_bc_change_maj} ({sd_bc_maj})\")\n",
    "    print(f\"Minority: {avg_bc_change_min} ({sd_bc_min})\")\n",
    "    print(\"\")\n",
    "    print('CC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_cc_change_maj} ({sd_cc_maj})\")\n",
    "    print(f\"Minority: {avg_cc_change_min} ({sd_cc_min})\")\n",
    "    print(\"\")\n",
    "    print('EVC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_evc_change_maj} ({sd_evc_maj})\")\n",
    "    print(f\"Minority: {avg_evc_change_min} ({sd_evc_min})\")\n",
    "    print(\"\")\n",
    "    print('Norm EVC Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_evcn_change_maj} ({sd_evcn_maj})\")\n",
    "    print(f\"Minority: {avg_evcn_change_min} ({sd_evcn_min})\")\n",
    "    print(\"\")\n",
    "    print('PR Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_pr_change_maj} ({sd_pr_maj})\")\n",
    "    print(f\"Minority: {avg_pr_change_min} ({sd_pr_min})\")\n",
    "    print(\"\")\n",
    "    print('Clustering Coeff Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_ccoeff_change_maj} ({sd_ccoeff_maj})\")\n",
    "    print(f\"Minority: {avg_ccoeff_change_min} ({sd_ccoeff_min})\")\n",
    "    print(\"\")\n",
    "    print('Avg. Shortest Path Changes')\n",
    "    print(\"---------------------------\")\n",
    "    print(f\"Majority: {avg_aspl_change_maj} ({sd_aspl_maj})\")\n",
    "    print(f\"Minority: {avg_aspl_change_min} ({sd_aspl_min})\")\n",
    "    print(\"\")\n",
    "    # print(f\"Avg. overlap: {np.round(np.average(overlaps), round)}\")\n",
    "\n",
    "    results = {'pct_changes': {'min': changes_dict_min, 'maj': changes_dict_maj}, \n",
    "               'frac_mins': [frac_mins_list, sd_frac_min], 'edge_types': edge_type_dict, \n",
    "               'recommended_nodes': recommended_nodes_dict}\n",
    "    if save_results:\n",
    "        \n",
    "        with open(save_as, 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "        print('')\n",
    "        print(f'Results saved to disk as {save_as}')\n",
    "\n",
    "    return results\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation_group_ratios(network_file='', network_obj=None, recommended_nodes=[], class_name='m'):\n",
    "    \n",
    "    if network_file == '':\n",
    "        network = network_obj\n",
    "    else:\n",
    "        network = read_pkl_graph(network_file)\n",
    "\n",
    "    all_nodes = network.nodes()\n",
    "    shares_min = []\n",
    "    shares_maj = []\n",
    "\n",
    "\n",
    "    for i in recommended_nodes.keys():\n",
    "\n",
    "        count_maj = 0\n",
    "        count_min = 0\n",
    "       \n",
    "\n",
    "        for tup in recommended_nodes[i]:\n",
    "            node = tup[-1]\n",
    "            if all_nodes[node][class_name] == 0:\n",
    "                count_maj += 1\n",
    "            elif all_nodes[node][class_name] == 1:\n",
    "                count_min += 1\n",
    "\n",
    "        shares_min.append(count_min/len(recommended_nodes[i])*100)\n",
    "        shares_maj.append(count_maj/len(recommended_nodes[i])*100)\n",
    "\n",
    "    avg_recomm_share_min, recomm_sd_min = np.round(np.average(shares_min), 5), np.round(np.std(shares_min), 5)\n",
    "    avg_recomm_share_maj, recomm_sd_maj = np.round(np.average(shares_maj), 5), np.round(np.std(shares_maj), 5)\n",
    "\n",
    "    \n",
    "    return avg_recomm_share_min, recomm_sd_min, avg_recomm_share_maj, recomm_sd_maj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_clustering_coefficient(network_file='', network_obj=None, mode='zero', class_name='m', group_names=[1, 0], verbose=False):\n",
    "\n",
    "    def get_nodes_per_class(network, class_name='m', group_names=[1, 0]):\n",
    "        \n",
    "        # class_\n",
    "        if class_name == 'm':\n",
    "            min_nodes_ig = network.vs.select(m=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(m=group_names[-1])\n",
    "\n",
    "        elif class_name == 'class_':\n",
    "            min_nodes_ig = network.vs.select(class_=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(class_=group_names[-1])\n",
    "\n",
    "        elif class_name == 'age':\n",
    "            min_nodes_ig = network.vs.select(age=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(age=group_names[-1])\n",
    "\n",
    "        return min_nodes_ig, maj_nodes_ig\n",
    "    \n",
    "    if network_file == '':\n",
    "        network = network_obj\n",
    "    else:\n",
    "        network = read_pkl_graph(network_file)\n",
    "\n",
    "    network_ig = nx_to_ig(network)\n",
    "\n",
    "    min_nodes, maj_nodes = get_nodes_per_class(network_ig, class_name=class_name, group_names=group_names)\n",
    "\n",
    "    # mode: how to treat nodes with degree < 2\n",
    "    transitivity = network_ig.transitivity_local_undirected(mode=mode)\n",
    "    network_ig.vs['clustering_coeff'] = transitivity\n",
    "\n",
    "    min_clustering_coeff = np.average(np.array(min_nodes.get_attribute_values('clustering_coeff')))\n",
    "    maj_clustering_coeff = np.average(np.array(maj_nodes.get_attribute_values('clustering_coeff')))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Min clustering coeff: {min_clustering_coeff}')\n",
    "        print(f'Maj clustering coeff: {maj_clustering_coeff}')\n",
    "\n",
    "    return min_clustering_coeff, maj_clustering_coeff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sp_lengths(network_file='', network_obj=None, mode='out', class_name='m', group_names=[1, 0], verbose=False):\n",
    "\n",
    "    def get_nodes_per_class(network, class_name='m', group_names=['young', 'old']):\n",
    "        \n",
    "        # class_\n",
    "        if class_name == 'm':\n",
    "            min_nodes_ig = network.vs.select(m=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(m=group_names[-1])\n",
    "\n",
    "        elif class_name == 'class_':\n",
    "            min_nodes_ig = network.vs.select(class_=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(class_=group_names[-1])\n",
    "\n",
    "        elif class_name == 'age':\n",
    "            min_nodes_ig = network.vs.select(age=group_names[0])\n",
    "            maj_nodes_ig = network.vs.select(age=group_names[-1])\n",
    "\n",
    "        return min_nodes_ig, maj_nodes_ig\n",
    "    \n",
    "    if network_file == '':\n",
    "        network = network_obj\n",
    "    else:\n",
    "        network = read_pkl_graph(network_file)\n",
    "    network_ig = nx_to_ig(network)\n",
    "\n",
    "    shortest_path_lengths = network_ig.shortest_paths(mode=mode)\n",
    "    arr = np.array(shortest_path_lengths)\n",
    "    arr[arr==inf] = 0\n",
    "    avg_sp_lens = [np.average(lens) for lens in arr]\n",
    "\n",
    "    min_nodes, maj_nodes = get_nodes_per_class(network_ig, class_name=class_name, group_names=group_names)\n",
    "\n",
    "    network_ig.vs['avg_sp_len'] = avg_sp_lens\n",
    "\n",
    "    min_avg_sp = np.average(np.array(min_nodes.get_attribute_values('avg_sp_len')))\n",
    "    maj_avg_sp = np.average(np.array(maj_nodes.get_attribute_values('avg_sp_len')))\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Min avg sp length: {min_avg_sp}')\n",
    "        print(f'Maj avg sp length: {maj_avg_sp}')\n",
    "\n",
    "    return min_avg_sp, maj_avg_sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist_per_group(G, attribute='', group_names=['', ''], group_labels=[1, 0]):\n",
    "    minority = [G.degree(n[0]) for n in G.nodes(data=True) if n[-1][attribute] == group_labels[0]]\n",
    "    majority = [G.degree(n[0]) for n in G.nodes(data=True) if n[-1][attribute] == group_labels[1]]\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, sharey=True)\n",
    "    fig.suptitle('Degree distribution by group')\n",
    "    \n",
    "    sns.histplot(ax=ax[0], data=minority)\n",
    "    # ax[0].set_ylabel('test')\n",
    "    ax[0].set_xlabel('Degree')\n",
    "    ax[0].set_title(f'{group_names[0]} (minority)')\n",
    "\n",
    "    sns.histplot(ax=ax[1], data=majority)\n",
    "    ax[1].set_xlabel('Degree')\n",
    "    ax[1].set_title(f'{group_names[1]} (majority)')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_ratios(network_obj, attribute='', group_names=['', ''], group_labels=[1, 0]):\n",
    "\n",
    "    minority = [n for n in network_obj.nodes(data=True) if n[-1][attribute] == group_labels[0]]\n",
    "    majority = [n for n in network_obj.nodes(data=True) if n[-1][attribute] == group_labels[1]]\n",
    "\n",
    "    print(f'# {group_names[0]} nodes (minority): {len(minority)} ({np.round(len(minority)/(len(minority)+len(majority))*100, 2)}%)')\n",
    "    print(f'# {group_names[1]} nodes (majority): {len(majority)} ({np.round(len(majority)/(len(minority)+len(majority))*100, 2)}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    sns.histplot(degrees)\n",
    "    plt.xlabel('Degree')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enum_vnum(network):\n",
    "    print(f'Number of edges: {network.number_of_edges()}')\n",
    "    print(f'Number of nodes: {network.number_of_nodes()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_nodes(network, name_as_str=True):\n",
    "\n",
    "    old = list(network.nodes())\n",
    "    new = list(range(0, len(old)))\n",
    "\n",
    "    if name_as_str:\n",
    "        old_str = [str(i) for i in old]\n",
    "        mapping1 = dict(zip(old, old_str))\n",
    "    else:\n",
    "        mapping1 = dict(zip(old, old))\n",
    "    nx.set_node_attributes(network, values=mapping1, name='name')\n",
    "\n",
    "    mapping2 = dict(zip(old, new))\n",
    "\n",
    "    return nx.relabel_nodes(network, mapping2, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_labels = nx.get_node_attributes(network, 'm')\n",
    "# color_map = {1:'#446df6', 0: '#93032e'}\n",
    "# nx.draw_networkx(\n",
    "#     G=network,\n",
    "#     pos=nx.spring_layout(network),\n",
    "#     labels=class_labels,\n",
    "#     with_labels=False,\n",
    "#     alpha=0.8,\n",
    "#     node_size=2,\n",
    "#     arrowsize=0.01,\n",
    "#     width=0.01,\n",
    "#     node_color=[color_map[network.nodes()[node]['m']] for node in network]\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "# pos=nx.spring_layout(network, iterations=1, scale=15)\n",
    "# class_labels = nx.get_node_attributes(network, 'm')\n",
    "# color_map = {1:'#446df6', 0: '#93032e'}\n",
    "\n",
    "# nx.draw_networkx(\n",
    "#     G=network,\n",
    "#     pos=pos,\n",
    "#     labels=class_labels,\n",
    "#     with_labels=False,\n",
    "#     alpha=0.8,\n",
    "#     node_size=2,\n",
    "#     arrowsize=0.01,\n",
    "#     width=0.01,\n",
    "#     node_color=[color_map[network.nodes()[node]['m']] for node in network]\n",
    "# )\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(r'C:\\Users\\timod\\Box\\University Documents\\Master\\UvA\\Thesis\\work\\thesis-opinion-dynamics\\nx28_2000nodes_topk_no_del.pkl', 'rb') as f:\n",
    "#     results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network82_rnd = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82_rnd = reindex_nodes(network82_rnd)\n",
    "network82_results_2000nodes_rnd = run_n_simulations(network_obj=network82_rnd, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='random', save_results=True, save_as='nx82_2000nodes_rnd.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "# add top k instead of sample from top 10\n",
    "network82_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "network82_results_2000nodes_new = run_n_simulations(network_obj=network82_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx82_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "# add top k instead of sample from top 10\n",
    "network82_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0802.pkl')\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "network82_results_2000nodes_old = run_n_simulations(network_obj=network82_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx82_2000nodes_old.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 99.76753% (0.05023)\n",
      "Avg. share of min: 0.23247% (0.05023)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 0.00708% (0.00867)\n",
      "mM: 20.21404% (1.11704)\n",
      "MM: 79.55349% (1.09188)\n",
      "Mm: 0.22539% (0.04191)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 0.63237% (0.05722)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: 0.06856 (0.0108)\n",
      "Minority: -4.88439 (0.76957)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.68864 (0.18202)\n",
      "Minority: -6.69622 (1.70562)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.98844 (0.02472)\n",
      "Minority: 1.46117 (0.0348)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -13.94934 (0.5276)\n",
      "Minority: -16.86817 (1.41452)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: 0.18971 (0.11389)\n",
      "Minority: -3.20169 (1.92213)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: 0.07835 (0.05348)\n",
      "Minority: -1.7499 (1.1946)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 41.04229 (2.21362)\n",
      "Minority: 63.23981 (4.7437)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.74007 (0.20423)\n",
      "Minority: -1.87976 (0.18537)\n",
      "\n",
      "\n",
      "Results saved to disk as nx82_2000nodes_ix_evc_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "# intervention\n",
    "# network82_topk = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "\n",
    "network82_topk = nx.read_gpickle(config.ROOT + \"nx_DPAH_10000_0208_0802.pkl\")\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network82_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network82_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network82_results_2000nodes_ix = run_n_simulations(network_obj=network82_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx82_2000nodes_ix_evc_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 92.64462% (0.50933)\n",
      "Avg. share of min: 7.35538% (0.50933)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 7.09824% (0.49143)\n",
      "mM: 13.39435% (0.6588)\n",
      "MM: 79.25027% (0.92702)\n",
      "Mm: 0.25714% (0.03344)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 7.503% (0.53056)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: -0.34055 (0.02542)\n",
      "Minority: 24.263 (1.81092)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.57672 (0.15848)\n",
      "Minority: 3.2962 (1.58749)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.84821 (0.02287)\n",
      "Minority: 0.79357 (0.04517)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -14.10073 (3.5815)\n",
      "Minority: -14.83065 (3.1316)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: 0.04589 (0.08353)\n",
      "Minority: -0.77446 (1.40979)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: -0.07897 (0.04259)\n",
      "Minority: 1.76376 (0.95138)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 38.87346 (1.51909)\n",
      "Minority: 42.52049 (2.47888)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.46313 (0.14509)\n",
      "Minority: -1.34556 (0.18299)\n",
      "\n",
      "\n",
      "Results saved to disk as nx82_2000nodes_ix_evc_min_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "# intervention\n",
    "# network82_topk = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")\n",
    "\n",
    "network82_topk = nx.read_gpickle(config.ROOT + \"nx_DPAH_10000_0208_0802.pkl\")\n",
    "network82_topk = reindex_nodes(network82_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network82_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network82_topk), to_add=top_min_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "# for which nodes / which nodes are added\n",
    "network82_results_2000nodes_ix = run_n_simulations(network_obj=network82_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx82_2000nodes_ix_evc_min_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network88_rnd = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_rnd = reindex_nodes(network88_rnd)\n",
    "network88_results_2000nodes_rnd = run_n_simulations(network_obj=network88_rnd, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='random', save_results=True, save_as='nx88_2000nodes_rnd.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "network88_results_2000nodes_new = run_n_simulations(network_obj=network88_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx88_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "# add top k instead of sample from top 10\n",
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "network88_results_2000nodes_old = run_n_simulations(network_obj=network88_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx88_2000nodes_old.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 97.3677% (0.12817)\n",
      "Avg. share of min: 2.6323% (0.12817)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 1.11063% (0.08991)\n",
      "mM: 18.38749% (0.66317)\n",
      "MM: 78.98021% (0.71257)\n",
      "Mm: 1.52166% (0.1252)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 3.91826% (0.1638)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: 0.50791 (0.0251)\n",
      "Minority: -3.93457 (0.19445)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.95121 (0.20547)\n",
      "Minority: -4.91555 (1.61244)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 1.05845 (0.00727)\n",
      "Minority: 1.44118 (0.03065)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -1.80456 (4.77942)\n",
      "Minority: -6.20263 (3.9266)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: 0.91393 (0.16841)\n",
      "Minority: -3.57312 (0.65841)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: 0.51353 (0.07327)\n",
      "Minority: -5.10907 (0.72891)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 36.70619 (1.41704)\n",
      "Minority: 52.96395 (3.31595)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.969 (0.28122)\n",
      "Minority: -2.45915 (0.3041)\n",
      "\n",
      "\n",
      "Results saved to disk as nx88_2000nodes_ix_evc_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network88_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network88_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network88_results_2000nodes_ix = run_n_simulations(network_obj=network88_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx88_2000nodes_ix_evc_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 90.51611% (0.29094)\n",
      "Avg. share of min: 9.48389% (0.29094)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 7.97504% (0.32658)\n",
      "mM: 11.64768% (0.76712)\n",
      "MM: 78.86843% (1.00153)\n",
      "Mm: 1.50885% (0.04291)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 9.83579% (0.27494)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: 0.08604 (0.03126)\n",
      "Minority: -0.66655 (0.24212)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -2.3936 (0.25302)\n",
      "Minority: -1.84371 (0.76283)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.94798 (0.0353)\n",
      "Minority: 1.00863 (0.06144)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -1.32525 (2.27742)\n",
      "Minority: -4.18344 (2.20184)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: 0.59334 (0.07894)\n",
      "Minority: -2.31975 (0.30864)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: 0.17242 (0.09287)\n",
      "Minority: -1.71541 (0.92392)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 36.111 (2.13477)\n",
      "Minority: 38.92078 (1.59927)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -2.01254 (0.14195)\n",
      "Minority: -2.06076 (0.29059)\n",
      "\n",
      "\n",
      "Results saved to disk as nx88_2000nodes_ix_evc_min_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network88_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0808.pkl')\n",
    "network88_topk = reindex_nodes(network88_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network88_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network88_topk), to_add=top_min_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network88_results_2000nodes_ix = run_n_simulations(network_obj=network88_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx88_2000nodes_ix_evc_min_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network28_rnd = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_rnd = reindex_nodes(network28_rnd)\n",
    "network28_results_2000nodes_rnd = run_n_simulations(network_obj=network28_rnd, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='random', save_results=True, save_as='nx28_2000nodes_rnd.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "network28_results_2000nodes_new = run_n_simulations(network_obj=network28_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx28_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "# add top k instead of sample from top 10\n",
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "network28_results_2000nodes_old = run_n_simulations(network_obj=network28_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx28_2000nodes_old.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 0.49341% (0.07529)\n",
      "Avg. share of min: 99.50659% (0.07529)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 19.74603% (0.28286)\n",
      "mM: 0.49341% (0.07529)\n",
      "MM: 0.0% (0.0)\n",
      "Mm: 79.76056% (0.29072)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 99.35333% (0.11062)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: -4.9575 (0.2063)\n",
      "Minority: 0.48677 (0.02026)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -4.35397 (1.91046)\n",
      "Minority: -2.90799 (0.40692)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 1.79888 (0.02366)\n",
      "Minority: 1.5272 (0.05902)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: 2.78548 (1.36395)\n",
      "Minority: 4.85557 (1.02996)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: -1.90227 (0.4459)\n",
      "Minority: 0.07641 (0.01791)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: -1.56679 (0.42902)\n",
      "Minority: 0.41158 (0.1127)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 13.32864 (1.04364)\n",
      "Minority: 15.28107 (1.29535)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -3.27089 (0.67774)\n",
      "Minority: -3.29872 (0.67572)\n",
      "\n",
      "\n",
      "Results saved to disk as nx28_2000nodes_ix_evc_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network28_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network28_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network28_results_2000nodes_ix = run_n_simulations(network_obj=network28_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx28_2000nodes_ix_evc_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 0.00352% (0.00703)\n",
      "Avg. share of min: 99.99648% (0.00703)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 19.4197% (0.77455)\n",
      "mM: 0.00352% (0.00703)\n",
      "MM: 0.0% (0.0)\n",
      "Mm: 80.57679% (0.7723)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 99.98531% (0.00613)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: -5.25503 (0.20667)\n",
      "Minority: 0.51598 (0.02029)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -4.00356 (0.54896)\n",
      "Minority: -2.81385 (0.37893)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 1.8068 (0.02213)\n",
      "Minority: 1.51292 (0.05349)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: 2.68801 (1.14972)\n",
      "Minority: 4.36519 (0.97952)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: -1.54738 (0.2609)\n",
      "Minority: 0.06216 (0.01048)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: -1.63094 (0.50843)\n",
      "Minority: 0.42844 (0.13356)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 12.64599 (0.79033)\n",
      "Minority: 13.93822 (0.53136)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -3.11204 (0.40869)\n",
      "Minority: -3.10115 (0.40891)\n",
      "\n",
      "\n",
      "Results saved to disk as nx28_2000nodes_ix_evc_min_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network28_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0208.pkl')\n",
    "network28_topk = reindex_nodes(network28_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network28_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network28_topk), to_add=top_min_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network28_results_2000nodes_ix = run_n_simulations(network_obj=network28_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx28_2000nodes_ix_evc_min_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network22_rnd = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_rnd = reindex_nodes(network22_rnd)\n",
    "network22_results_2000nodes_rnd = run_n_simulations(network_obj=network22_rnd, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='random', save_results=True, save_as='nx22_2000nodes_rnd.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "network22_results_2000nodes_new = run_n_simulations(network_obj=network22_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx22_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "# add top k instead of sample from top 10\n",
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "network22_results_2000nodes_old = run_n_simulations(network_obj=network22_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx22_2000nodes_old.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 36.72259% (0.3945)\n",
      "Avg. share of min: 63.27741% (0.3945)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 14.7143% (0.45475)\n",
      "mM: 5.15202% (0.37962)\n",
      "MM: 31.57058% (0.20961)\n",
      "Mm: 48.56311% (0.71371)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 49.40474% (0.35835)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: 2.04889 (0.05163)\n",
      "Minority: -0.77377 (0.01949)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.41523 (0.36605)\n",
      "Minority: -1.42521 (0.30023)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.94041 (0.04444)\n",
      "Minority: 1.45229 (0.08452)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: 0.44956 (3.74427)\n",
      "Minority: 1.21594 (3.53414)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: -0.61984 (0.40332)\n",
      "Minority: 0.14633 (0.09522)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: -0.89553 (0.43836)\n",
      "Minority: 0.90321 (0.44212)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 23.45853 (2.07592)\n",
      "Minority: 46.50668 (4.90782)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.49862 (0.28689)\n",
      "Minority: -1.56281 (0.31854)\n",
      "\n",
      "\n",
      "Results saved to disk as nx22_2000nodes_ix_evc_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network22_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network22_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network22_results_2000nodes_ix = run_n_simulations(network_obj=network22_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx22_2000nodes_ix_evc_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 36.4199% (0.36355)\n",
      "Avg. share of min: 63.5801% (0.36355)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 15.61475% (0.70948)\n",
      "mM: 4.30575% (0.36826)\n",
      "MM: 32.11415% (0.57247)\n",
      "Mm: 47.96535% (0.48883)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 49.62718% (0.25152)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: 2.03357 (0.21482)\n",
      "Minority: -0.76798 (0.08112)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.30895 (0.19192)\n",
      "Minority: -0.92201 (0.12056)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.9204 (0.03257)\n",
      "Minority: 1.45898 (0.06478)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -0.49215 (2.99622)\n",
      "Minority: 0.1759 (3.1404)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: -0.53665 (0.27952)\n",
      "Minority: 0.12669 (0.06599)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: -1.03993 (0.43302)\n",
      "Minority: 1.04885 (0.43673)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 24.99297 (2.10736)\n",
      "Minority: 47.74447 (4.30204)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.1957 (0.1128)\n",
      "Minority: -1.25358 (0.13434)\n",
      "\n",
      "\n",
      "Results saved to disk as nx22_2000nodes_ix_evc_min_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network22_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0202.pkl')\n",
    "network22_topk = reindex_nodes(network22_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network22_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network22_topk), to_add=top_min_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network22_results_2000nodes_ix = run_n_simulations(network_obj=network22_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx22_2000nodes_ix_evc_min_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### n55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network55_rnd = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_rnd = reindex_nodes(network55_rnd)\n",
    "network55_results_2000nodes_rnd = run_n_simulations(network_obj=network55_rnd, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='random', save_results=True, save_as='nx55_2000nodes_rnd.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "network55_results_2000nodes_new = run_n_simulations(network_obj=network55_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='new', save_results=True, save_as='nx55_2000nodes_new.pkl', \n",
    "                                            to_txt=True, salsa_new=False, quick_test=False, to_pick=5, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider 2000 nodes\n",
    "# add top k instead of sample from top 10\n",
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "network55_results_2000nodes_old = run_n_simulations(network_obj=network55_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx55_2000nodes_old.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 83.33438% (0.31161)\n",
      "Avg. share of min: 16.66562% (0.31161)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 2.24585% (0.17286)\n",
      "mM: 17.61261% (0.56891)\n",
      "MM: 65.72177% (0.732)\n",
      "Mm: 14.41976% (0.38807)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 17.91096% (0.19961)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: 0.21838 (0.02043)\n",
      "Minority: -0.90731 (0.0849)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.32877 (0.19416)\n",
      "Minority: -2.48584 (0.29375)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.83623 (0.01667)\n",
      "Minority: 1.12354 (0.04121)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -12.90163 (2.25805)\n",
      "Minority: -13.43051 (2.17121)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: 0.2947 (0.1415)\n",
      "Minority: -0.31209 (0.14985)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: 0.1988 (0.09325)\n",
      "Minority: -0.82638 (0.38763)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 37.32867 (1.13475)\n",
      "Minority: 46.15276 (4.10223)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.40407 (0.17984)\n",
      "Minority: -1.6676 (0.1805)\n",
      "\n",
      "\n",
      "Results saved to disk as nx55_2000nodes_ix_evc_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network55_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network55_topk), to_add=top_maj_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network55_results_2000nodes_ix = run_n_simulations(network_obj=network55_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx55_2000nodes_ix_evc_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished run 0\n",
      "Finished run 1\n",
      "Finished run 2\n",
      "Finished run 3\n",
      "Finished run 4\n",
      "\n",
      "Mean stats and standard deviation for 5 runs\n",
      "---------------------------------------------------\n",
      "\n",
      "Added Nodes Min/Maj Fraction\n",
      "---------------------------\n",
      "Avg. share of maj: 76.4743% (0.77744)\n",
      "Avg. share of min: 23.5257% (0.77744)\n",
      "\n",
      "Edge types\n",
      "---------------------------\n",
      "mm: 9.47772% (0.56532)\n",
      "mM: 10.95061% (0.3961)\n",
      "MM: 65.52369% (1.04388)\n",
      "Mm: 14.04798% (0.3192)\n",
      "\n",
      "Top 10 Minority Fraction Recommendations\n",
      "---------------------------\n",
      "Avg. minority fraction: 23.01726% (0.33781)\n",
      "\n",
      "DC Changes\n",
      "---------------------------\n",
      "Majority: -0.27894 (0.04702)\n",
      "Minority: 1.15888 (0.19534)\n",
      "\n",
      "BC Changes\n",
      "---------------------------\n",
      "Majority: -1.37949 (0.32508)\n",
      "Minority: -0.60346 (0.77188)\n",
      "\n",
      "CC Changes\n",
      "---------------------------\n",
      "Majority: 0.78281 (0.02184)\n",
      "Minority: 1.02489 (0.04038)\n",
      "\n",
      "EVC Changes\n",
      "---------------------------\n",
      "Majority: -12.99652 (2.02144)\n",
      "Minority: -12.51906 (1.91974)\n",
      "\n",
      "Norm EVC Changes\n",
      "---------------------------\n",
      "Majority: -0.26724 (0.09761)\n",
      "Minority: 0.28301 (0.10336)\n",
      "\n",
      "PR Changes\n",
      "---------------------------\n",
      "Majority: -0.22375 (0.08401)\n",
      "Minority: 0.93008 (0.34919)\n",
      "\n",
      "Clustering Coeff Changes\n",
      "---------------------------\n",
      "Majority: 37.33498 (0.75089)\n",
      "Minority: 40.05304 (3.66217)\n",
      "\n",
      "Avg. Shortest Path Changes\n",
      "---------------------------\n",
      "Majority: -1.19526 (0.16178)\n",
      "Minority: -1.25204 (0.18429)\n",
      "\n",
      "\n",
      "Results saved to disk as nx55_2000nodes_ix_evc_min_min_j10_05edges.pkl\n"
     ]
    }
   ],
   "source": [
    "network55_topk = nx.read_gpickle(config.ROOT + 'nx_DPAH_10000_0208_0505.pkl')\n",
    "network55_topk = reindex_nodes(network55_topk)\n",
    "\n",
    "top_maj_evc, top_min_evc = get_topj_evc_nodes(network_obj=network55_topk, j=10, to_ig=True, mode='both')\n",
    "results = get_intervention_recommendations(A=nx.adjacency_matrix(network55_topk), to_add=top_min_evc, p=0.85, top=10, num_cores=-1, network_obj=None)\n",
    "\n",
    "network55_results_2000nodes_ix = run_n_simulations(network_obj=network55_topk, n=5, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=True, save_as='nx55_2000nodes_ix_evc_min_min_j10_05edges.pkl', \n",
    "                                            to_txt=True, salsa_new=True, quick_test=False, to_pick=3, \n",
    "                                            consider_all_nodes=False, num_to_consider=2000, add_top_k=True,\n",
    "                                            intervene=True, strat='evc', metric_mode='both', j=5, recomms=results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pokec network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokec_sample = nx.read_gpickle(config.ROOT+'POKEC-A21/nx_pokec_sample_fabbri_netfile_relabeled_attributes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attach age data to nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokec_nodes_age = pd.read_csv(config.ROOT+'POKEC-A21/graph_nodes_by_age.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids_all = np.array(list(pokec_nodes_age['node']))\n",
    "node_age_all = np.array(list(pokec_nodes_age['age']))\n",
    "\n",
    "mapping_all = dict(zip(node_ids_all, node_age_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokec_sample_nodes = np.array(list(pokec_sample.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = []\n",
    "\n",
    "for node in node_ids_all:\n",
    "\n",
    "    if node in pokec_sample_nodes:\n",
    "        id_list.append(node)\n",
    "        if len(id_list) % 20000 == 0:\n",
    "            print(len(id_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_list = []\n",
    "\n",
    "for node in node_ids_all:\n",
    "\n",
    "    if node in pokec_sample_nodes:\n",
    "        age_list.append(mapping_all[node])\n",
    "        if len(age_list) % 20000 == 0:\n",
    "            print(len(age_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_age_mapping = dict(zip(id_list, age_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(pokec_sample, values=id_age_mapping, name='age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(pokec_sample, config.ROOT+'POKEC-A21/nx_pokec_sample_fabbri_netfile_relabeled_attributes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_vnum(pokec_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = [print(x) for x,y in pokec_sample.nodes(data=True) if 'age' not in y.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_group_ratios(pokec_sample, attribute='age', group_names=['Young', 'Old'], group_labels=['young', 'old'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prl = reindex_nodes(pokec_sample, name_as_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_clustering_coefficient(network_obj=prl, mode='zero', class_name='age', group_names=['young', 'old'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_sp_lengths(network_obj=prl, mode='out', class_name='age', group_names=['young', 'old'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokec_changes, pokec_nodes = run_n_simulations(network_obj=prl, n=3, \n",
    "                                            num_to_sample=20000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='age', min_label='young', maj_label='old',\n",
    "                                            strategy='bulk', save_results=True, save_as='pokec_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendation_group_ratios(network_obj=prl,\n",
    "                                recommended_nodes=pokec_nodes,\n",
    "                                class_name='age',\n",
    "                                deleted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blogs network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = nx.read_gpickle(config.ROOT + \"blogs_attributed_network_anon.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_vnum(blogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_group_ratios(blogs, attribute='m', group_names=['Left-leaning', 'Right-leaning'], group_labels=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist_per_group(blogs, attribute='m', group_names=['Left-leaning', 'Right-leaning'], group_labels=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brl = reindex_nodes(blogs, name_as_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_clustering_coefficient(network_obj=brl, mode='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_sp_lengths(network_obj=brl, mode='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_elements(L):\n",
    "    start, end = L[0], L[-1]\n",
    "    return sorted(set(range(start, end + 1)).difference(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_changes, blogs_nodes = run_n_simulations(network_obj=brl, n=2, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='consecutively', save_results=False, save_as='blogs_results_test.pkl', to_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendation_group_ratios(network_obj=brl,\n",
    "                                recommended_nodes=blogs_nodes,\n",
    "                                class_name='m',\n",
    "                                deleted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hate network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate = nx.read_gpickle(config.ROOT + \"hate_attributed_network_anon.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_vnum(hate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_group_ratios(hate, attribute='m', group_names=['Hateful', 'Normal'], group_labels=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist_per_group(hate, attribute='m', group_names=['Hateful', 'Normal'], group_labels=[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# majority = [hate.degree(n[0]) for n in hate.nodes(data=True) if n[-1]['m'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrl = reindex_nodes(hate, name_as_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_clustering_coefficient(network_obj=hrl, mode='zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_avg_sp_lengths(network_obj=hrl, mode='out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_changes, hate_nodes = run_n_simulations(network_obj=hrl, n=10, \n",
    "                                            num_to_sample=2000, delete_edges=True, verbose=False, round=5, \n",
    "                                            remove_zero_degree_nodes=True, class_name='m', min_label=1, maj_label=0,\n",
    "                                            strategy='random', save_results=True, save_as='hate_results_random.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendation_group_ratios(network_obj=hrl,\n",
    "                                recommended_nodes=hate_nodes,\n",
    "                                class_name='m',\n",
    "                                deleted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from Wagner et al. (2017):\n",
    "\n",
    "This strategy samples nodes by walking through the network. The walker starts at a random node in the network and chooses in each step one out-going link randomly and traverses it. All visited nodes are then added to the sample until K nodes have been added. A teleport probability can be set for teleporting to another random node in the network instead of traversing a link in this iteration; we use 0.15 throughout this work. The sampled network then contains these K nodes and all links between them. This technique of sampling is usually used in online social networks such as Facebook or Twitter, in\n",
    "which retrieving information about the whole population is overwhelming and computationally costly, but we can access and navigate the original network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick test network\n",
    "g = nx.scale_free_graph(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RandomWalker:\n",
    "    def __init__(self, full_network, sample_size, with_attribute=False, attribute=''):\n",
    "        self.full_network = full_network\n",
    "        self.sample_size = sample_size\n",
    "        self.sampled_network = None\n",
    "        self.undirected_G = nx.to_undirected(self.full_network)\n",
    "        self.with_attribute = with_attribute\n",
    "        self.attribute = attribute\n",
    "    \n",
    "    def add_distinct_node(self, node):\n",
    "        if node not in list(self.sampled_network.nodes()):\n",
    "            if self.with_attribute:\n",
    "                self.sampled_network.add_node(node, _class = self.full_network.nodes()[node][self.attribute])\n",
    "            else:\n",
    "                self.sampled_network.add_node(node)\n",
    "\n",
    "    def get_all_valid_edges_between_sampled_nodes(self):\n",
    "        \n",
    "        sampled_network_nodes = self.sampled_network.nodes()\n",
    "\n",
    "        for node_i in sampled_network_nodes:\n",
    "            for out_edge in self.full_network.out_edges(node_i):\n",
    "                if out_edge[-1] in sampled_network_nodes:\n",
    "                    self.sampled_network.add_edge(node_i, out_edge[-1])  \n",
    "\n",
    "        return self.sampled_network\n",
    "\n",
    "    def show_sample(self):\n",
    "        assert self.sampled_network != None\n",
    "        nx.draw(self.sampled_network)\n",
    "        plt.show()\n",
    "\n",
    "    def assert_connectedness(self):\n",
    "        temp_undir_G = self.sampled_network.copy().to_undirected()\n",
    "        z = (temp_undir_G.subgraph(c) for c in nx.connected_components(temp_undir_G))\n",
    "\n",
    "        b = 0\n",
    "        for i in z:\n",
    "            print(i)\n",
    "            b += 1\n",
    "        \n",
    "        return True if b == 1 else False\n",
    "        \n",
    "    def get_random_walk_sampled_network(self, teleportation_proba=0.15, verbose=False, print_every=100):\n",
    "    \n",
    "        # get list of all nodes in the network\n",
    "        all_nodes  = list(self.full_network.nodes())\n",
    "\n",
    "        # randomly choose 1 node as starting point\n",
    "        k_0 = np.random.choice(all_nodes, 1)[0]\n",
    "\n",
    "        # initialize empty, directed networkx graph\n",
    "        self.sampled_network = nx.DiGraph()\n",
    "        self.add_distinct_node(k_0)\n",
    "\n",
    "        i = self.sampled_network.number_of_nodes()\n",
    "\n",
    "        while i < self.sample_size:\n",
    "            \n",
    "            # get a list of all nodes with an in-edge from k_0\n",
    "            out_nodes_k0 = [tup[1] for tup in self.full_network.out_edges(k_0)]\n",
    "            \n",
    "            # draw random number to determine whether the walker traverses an edge or teleports\n",
    "            # or random.uniform\n",
    "            traversion_threshold = (np.random.randint(1, 100)) / 100\n",
    "\n",
    "            # traverse an edge with probability equal to 1-teleportation_proba\n",
    "            if traversion_threshold <= (1-teleportation_proba):\n",
    "\n",
    "                # if node k_0 has at least one outgoing edge, draw a random destination node\n",
    "                # and add node k_1 to the network\n",
    "                if len(out_nodes_k0) > 0:\n",
    "                    k_1 = np.random.choice(out_nodes_k0, 1)[0]\n",
    "                    self.add_distinct_node(k_1)\n",
    "                    self.add_distinct_node(k_0)\n",
    "                \n",
    "                # if node k_0 does not have any outgoing edges and is not yet in nodes, add node k_0 to graph\n",
    "                # in this case, pick new random k_1 node from all possible nodes (where k_1 !=  k_0)\n",
    "                else:\n",
    "                    k_1 = np.random.choice([node for node in all_nodes if node != k_0], 1)[0]\n",
    "                    self.add_distinct_node(k_1)\n",
    "                    self.add_distinct_node(k_0)\n",
    "            \n",
    "            # teleport to a random node\n",
    "            else:\n",
    "                # remove connected nodes from possible teleportation destinations\n",
    "                \n",
    "                # checking for descendants of one random out node of k_0 works because undirected graph is used\n",
    "                # in an undirected graph, using descendants on a node t returns all reachable nodes from t\n",
    "                # this is akin to just returning all nodes in a network, with the exception of disjointed nodes\n",
    "               \n",
    "                # if len(out_nodes_k0) > 0:\n",
    "                #     random_out_node = np.random.choice(out_nodes_k0, 1)[0]\n",
    "                #     all_descendants = nx.algorithms.descendants(self.undirected_G, random_out_node)\n",
    "                    \n",
    "                #     all_possible_nodes = list(all_descendants - set(out_nodes_k0))\n",
    "                #     all_possible_nodes.remove(k_0)\n",
    "                    \n",
    "                all_possible_nodes = [node for node in all_nodes if node not in list(self.sampled_network.nodes())]\n",
    "                # randomly pick a teleportation destination node\n",
    "                k_1 = np.random.choice(all_possible_nodes, 1)[0]\n",
    "                self.add_distinct_node(k_1)\n",
    "                self.add_distinct_node(k_0)\n",
    "\n",
    "            # update the new starting node to be the previous destination node\n",
    "            k_0 = k_1\n",
    "                \n",
    "\n",
    "            i = self.sampled_network.number_of_nodes()\n",
    "\n",
    "            if verbose:\n",
    "                if i % print_every == 0:\n",
    "                    print(f\"Added {i} nodes.\")\n",
    "        \n",
    "    \n",
    "        self.sampled_network = self.get_all_valid_edges_between_sampled_nodes()\n",
    "\n",
    "        \n",
    "        return self.sampled_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RandomWalker_iG:\n",
    "    def __init__(self, full_network, sample_size):\n",
    "        self.full_network = full_network\n",
    "        self.sample_size = sample_size\n",
    "        self.sampled_network = None\n",
    "    \n",
    "    def add_distinct_node(self, node, test=False):\n",
    "        if node not in self.sampled_network.vs()['name']:\n",
    "            if not test:\n",
    "                self.sampled_network.add_vertex(node, _class = self.full_network.vs['name'==node]['class'])\n",
    "            else:\n",
    "                self.sampled_network.add_vertex(node)\n",
    "\n",
    "\n",
    "    def get_all_valid_edges_between_sampled_nodes(self):\n",
    "        \n",
    "        sampled_network_nodes = self.sampled_network.vs()['name']\n",
    "\n",
    "        # for node_i in sampled_network_nodes:\n",
    "        #     for out_v in [self.full_network.vs[i]['name'] for i in self.full_network.neighbors(node_i, mode='out')]:\n",
    "        #         if out_v in sampled_network_nodes:\n",
    "        #             self.sampled_network.add_edge(node_i, out_v)  \n",
    "        \n",
    "        for node_i in sampled_network_nodes:\n",
    "    \n",
    "            out_vs = [self.full_network.vs[i]['name'] for i in self.full_network.neighbors(node_i, mode='out')]\n",
    "            intersection = list(set(sampled_network_nodes) & set(out_vs))\n",
    "            \n",
    "            for _ in intersection:\n",
    "                self.sampled_network.add_edge(node_i, _)\n",
    "\n",
    "        return self.sampled_network\n",
    "\n",
    "       \n",
    "    def get_random_walk_sampled_network(self, teleportation_proba=0.15, verbose=False, print_every=100, test=False):\n",
    "    \n",
    "        # get list of all nodes in the network\n",
    "        # all_nodes = [int(i) for i in self.full_network.vs()['name']]\n",
    "        all_nodes = self.full_network.vs()['name']\n",
    "        print(\"got all node names\")\n",
    "\n",
    "        # randomly choose 1 node as starting point\n",
    "        k_0 = np.random.choice(all_nodes, 1)[0]\n",
    "\n",
    "        # initialize empty, directed graph\n",
    "        self.sampled_network = ig.Graph(directed=True)\n",
    "        # self.sampled_network.add_vertex(k_0, _class = self.full_network.vs['name'==k_0]['class'])\n",
    "        self.sampled_network.add_vertex(k_0)\n",
    "        print(\"initiated digraph and added first k0\")\n",
    "\n",
    "        i = self.sampled_network.vcount()\n",
    "\n",
    "        while i < self.sample_size:\n",
    "            \n",
    "            # draw random number to determine whether the walker traverses an edge or teleports\n",
    "            traversion_threshold = (np.random.randint(1, 100)) / 100\n",
    "\n",
    "            # traverse an edge with probability equal to 1-teleportation_proba\n",
    "            if traversion_threshold <= (1-teleportation_proba):\n",
    "                \n",
    "                print(\"Step 1: traversed\")\n",
    "\n",
    "                # get a list of all nodes with an in-edge from k_0\n",
    "                out_nodes_k0_ids = self.full_network.neighbors(k_0, mode='out')\n",
    "                out_nodes_k0 = [self.full_network.vs[i]['name'] for i in out_nodes_k0_ids]\n",
    "\n",
    "                print(f\"Step 2: got out-nodes of k0; number of out-nodes: {len(out_nodes_k0)}\")\n",
    "\n",
    "                # if node k_0 has at least one outgoing edge, draw a random destination node\n",
    "                # and add node k_1 to the network\n",
    "                if len(out_nodes_k0) > 0:\n",
    "                    print(\"Step 3a: out-nodes k0 > 0; choosing k1\")\n",
    "                    k_1 = np.random.choice(out_nodes_k0, 1)[0]\n",
    "                    # self.sampled_network.add_vertex(k_1, _class = self.full_network.vs['name'==k_1]['class'])\n",
    "                    self.add_distinct_node(k_1, test=test)\n",
    "                    # self.sampled_network.add_vertex(k_1)\n",
    "                    print(\"Step 3b: add vertex k1\")\n",
    "                    # add k_0 if not already in nodes\n",
    "                    self.add_distinct_node(k_0, test=test)\n",
    "                \n",
    "                # if node k_0 does not have any outgoing edges and is not yet in nodes, add node k_0 to graph\n",
    "                # in this case, pick new random k_1 node from all possible nodes (where k_1 !=  k_0)\n",
    "                # TODO: determine if this is actually useful / needed\n",
    "                else:\n",
    "                    \n",
    "                    k_1 = np.random.choice([node for node in all_nodes if node != k_0], 1)[0]\n",
    "                    # self.sampled_network.add_vertex(k_1, _class = self.full_network.vs['name'==k_1]['class'])\n",
    "                    # self.sampled_network.add_vertex(k_1)\n",
    "                    self.add_distinct_node(k_1, test=test)\n",
    "                    print(\"Step 3c: out-nodes k0 < 0; randomly choosing new k1 and adding vertex\")\n",
    "                    # add k_0 if not already in nodes\n",
    "                    self.add_distinct_node(k_0, test=test)\n",
    "            \n",
    "            # teleport to a random node\n",
    "            else:\n",
    "                print(\"Step 1: teleportation\")\n",
    "                # setting order to a very high number and mode='all' ensures that all reachable node ids will be returned\n",
    "                all_possible_nodes_ids = self.full_network.neighborhood(vertices=k_0, order=1000, mode='all', mindist=2)\n",
    "                all_possible_nodes = [self.full_network.vs[i]['name'] for i in all_possible_nodes_ids]\n",
    "                print(\"Step 2: got all possible destination nodes\")\n",
    "\n",
    "                if len(all_possible_nodes) > 0:                \n",
    "                # randomly pick a teleportation destination node\n",
    "                    k_1 = np.random.choice(all_possible_nodes, 1)[0]\n",
    "                else:\n",
    "                    k_1 = np.random.choice([node for node in all_nodes if node != k_0], 1)[0]\n",
    "                \n",
    "                self.add_distinct_node(k_1, test=test)\n",
    "                # add k_0 if not already in nodes\n",
    "                self.add_distinct_node(k_0, test=test)\n",
    "                print(\"Step 3: teleported to k1 and added k1\")\n",
    "\n",
    "            # update the new starting node to be the previous destination node\n",
    "            k_0 = k_1\n",
    "            print(\"k1 now new k0, repeat\")\n",
    "\n",
    "            # i = self.sampled_network.number_of_nodes()\n",
    "            i = self.sampled_network.vcount()\n",
    "\n",
    "            if verbose:\n",
    "                if i % print_every == 0:\n",
    "                    print(f\"Added {i} nodes.\")\n",
    "        \n",
    "        print(\"Getting all valid edges between sampled nodes\")\n",
    "        self.sampled_network = self.get_all_valid_edges_between_sampled_nodes()\n",
    "        \n",
    "        return self.sampled_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distinct_node(node, test=False):\n",
    "    if node not in sampled_network.vs():\n",
    "        if not test:\n",
    "            sampled_network.add_vertex(node, _class = full_network.vs['name'==node]['class'])\n",
    "        else:\n",
    "            sampled_network.add_vertex(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_network.ecount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_network_nodes = sampled_network.vs()['name']\n",
    "\n",
    "# for node_i in sampled_network_nodes:\n",
    "#     for out_v in [self.full_network.vs[i]['name'] for i in self.full_network.neighbors(node_i, mode='out')]:\n",
    "#         if out_v in sampled_network_nodes:\n",
    "#             self.sampled_network.add_edge(node_i, out_v)  \n",
    "\n",
    "for node_i in sampled_network_nodes:\n",
    "\n",
    "    out_vs = [full_network.vs[i]['name'] for i in full_network.neighbors(node_i, mode='out')]\n",
    "    intersection = list(set(sampled_network_nodes) & set(out_vs))\n",
    "    \n",
    "    for _ in intersection:\n",
    "        sampled_network.add_edge(node_i, _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = read_pkl_graph(\"graph_no_attributes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_clean = G.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_p = load_profile_data(config.PROFILES_PATH)\n",
    "P = profile_data_to_df(_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_clean = P.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_na = P_clean[P_clean['age'].isna()]\n",
    "P_ng = P_clean[P_clean['gender'].isna()]\n",
    "\n",
    "zz = P_na['user_id'].tolist()\n",
    "yy = P_ng['user_id'].tolist()\n",
    "\n",
    "P_clean.dropna(inplace=True)\n",
    "P_clean = P_clean.astype({'user_id':str, 'gender':'int64', 'age':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = P_clean[P_clean['age'] == 0]['user_id'].tolist()\n",
    "P_clean = P_clean[P_clean['age'] != 0]\n",
    "\n",
    "all_dropped_nodes = yy + xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_clean.delete_vertices(all_dropped_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = G_clean.get_edgelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gnx = nx.from_edgelist([(names[x[0]], names[x[1]])\n",
    "                      for names in [G_clean.vs['name']]\n",
    "                      for x in mu], nx.DiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(Gnx, config.ROOT+'nx_graph_no_attributes.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_clean_nx = nx.read_gpickle(config.ROOT + 'nx_graph_no_attributes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G_clean_nx.number_of_nodes())\n",
    "print(G_clean_nx.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [node for node, degree in dict(G_clean_nx.degree()).items() if degree < 1]\n",
    "print(f'# nodes to remove: {len(remove)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_clean_nx.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = RandomWalker(full_network=G_clean_nx, sample_size=2000, with_attribute=False, attribute='')\n",
    "sample = rw.get_random_walk_sampled_network(teleportation_proba=0.15, verbose=True, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import itertools\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# class RandomWalker3:\n",
    "#     def __init__(self, full_network, sample_size, with_attribute=False, attribute=''):\n",
    "#         self.full_network = full_network\n",
    "#         self.sample_size = sample_size\n",
    "#         self.sampled_network = None\n",
    "#         self.undirected_G = nx.to_undirected(self.full_network)\n",
    "#         self.with_attribute = with_attribute\n",
    "#         self.attribute = attribute\n",
    "    \n",
    "#     def add_distinct_node(self, node):\n",
    "#         if node not in list(self.sampled_network.nodes()):\n",
    "#             if self.with_attribute:\n",
    "#                 self.sampled_network.add_node(node, _class = self.full_network.nodes()[node][self.attribute])\n",
    "#             else:\n",
    "#                 self.sampled_network.add_node(node)\n",
    "\n",
    "#     def get_all_valid_edges_between_sampled_nodes(self):\n",
    "        \n",
    "#         sampled_network_nodes = self.sampled_network.nodes()\n",
    "\n",
    "#         for node_i in sampled_network_nodes:\n",
    "#             for out_edge in self.full_network.out_edges(node_i):\n",
    "#                 if out_edge[-1] in sampled_network_nodes:\n",
    "#                     self.sampled_network.add_edge(node_i, out_edge[-1])  \n",
    "\n",
    "#         return self.sampled_network\n",
    "\n",
    "#     def show_sample(self):\n",
    "#         assert self.sampled_network != None\n",
    "#         nx.draw(self.sampled_network)\n",
    "#         plt.show()\n",
    "\n",
    "#     def assert_connectedness(self):\n",
    "#         temp_undir_G = self.sampled_network.copy().to_undirected()\n",
    "#         z = (temp_undir_G.subgraph(c) for c in nx.connected_components(temp_undir_G))\n",
    "\n",
    "#         b = 0\n",
    "#         for i in z:\n",
    "#             b += 1\n",
    "        \n",
    "#         return True if b == 1 else False\n",
    "        \n",
    "#     def get_random_walk_sampled_network(self, teleportation_proba=0.15, verbose=False, print_every=100):\n",
    "    \n",
    "#         # get list of all nodes in the network\n",
    "#         all_nodes  = list(self.full_network.nodes())\n",
    "\n",
    "#         # randomly choose 1 node as starting point\n",
    "#         k_0 = np.random.choice(all_nodes, 1)[0]\n",
    "\n",
    "#         # initialize empty, directed networkx graph\n",
    "#         self.sampled_network = nx.DiGraph()\n",
    "#         self.add_distinct_node(k_0)\n",
    "\n",
    "#         i = self.sampled_network.number_of_nodes()\n",
    "\n",
    "#         while i < self.sample_size:\n",
    "            \n",
    "#             # get a list of all nodes with an in-edge from k_0\n",
    "#             # out_nodes_k0 = [tup[1] for tup in self.full_network.out_edges(k_0)]\n",
    "#             out_nodes_k0 = list(nx.neighbors(self.undirected_G, k_0))\n",
    "\n",
    "#             if len(out_nodes_k0) > 0:\n",
    "#                 k_1 = np.random.choice(out_nodes_k0, 1)[0]\n",
    "#                 self.add_distinct_node(k_1)\n",
    "#                 self.add_distinct_node(k_0)\n",
    "            \n",
    "#             else:\n",
    "#                 k_1 = np.random.choice(list(self.sampled_network.nodes()), 1)[0]\n",
    "            \n",
    "#             k_0 = k_1\n",
    "\n",
    "#             i = self.sampled_network.number_of_nodes()\n",
    "\n",
    "#             if verbose:\n",
    "#                 if i % print_every == 0:\n",
    "#                     print(f\"Added {i} nodes.\")\n",
    "        \n",
    "#         self.sampled_network = self.get_all_valid_edges_between_sampled_nodes()\n",
    "\n",
    "        \n",
    "#         return self.sampled_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rwx = RandomWalker_iG(full_network=G_clean, sample_size=100)\n",
    "pokec_sample = rwx.get_random_walk_sampled_network(teleportation_proba=0, verbose=True, print_every=5, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw.show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    sns.histplot(degrees)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist(G2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_degree_dist_per_group(G, attribute=''):\n",
    "    group_0 = [G.degree(n[0]) for n in G.nodes(data=True) if n[-1][attribute] == 0]\n",
    "    group_1 = [G.degree(n[0]) for n in G.nodes(data=True) if n[-1][attribute] == 1]\n",
    "    \n",
    "    plt.subplot(1, 2, 1)  # 1 line, 2 rows, index nr 1 (first position in the subplot)\n",
    "    plt.hist(group_0)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.title('group_0')\n",
    "    plt.subplot(1, 2, 2)  # 1 line, 2 rows, index nr 2 (second position in the subplot)\n",
    "    plt.hist(group_1)\n",
    "    plt.xlabel('Degree')\n",
    "    plt.title('group_1')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_degree_dist(G):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    sns.histplot(degrees)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist_per_group(sample, attribute='_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.average_shortest_path_length(G2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.average_shortest_path_length(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTAL\n",
    "\n",
    "SAMPLE_SIZE = 100\n",
    "\n",
    "all_nodes  = [n for n in G.nodes()]\n",
    "k_0 = np.random.choice(all_nodes, 1)[0]\n",
    "\n",
    "sampled_network = nx.DiGraph()\n",
    "\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    \n",
    "    out_edges_k0 = [tup[1] for tup in G.out_edges(k_0)]\n",
    "    \n",
    "    traversion_proba = np.random.randint(1, 100)\n",
    "\n",
    "    if traversion_proba <= 85:\n",
    "        if len(out_edges_k0) > 0:\n",
    "            k_1 = np.random.choice(out_edges_k0, 1)[0]\n",
    "            sampled_network.add_edge(k_0, k_1)\n",
    "        else:\n",
    "            sampled_network.add_node(k_0)\n",
    "            k_1 = np.random.choice(all_nodes, 1)[0]\n",
    "    \n",
    "    else:\n",
    "        k_1 = np.random.choice(all_nodes, 1)[0]\n",
    "        sampled_network.add_node(k_1)\n",
    "        sampled_network.add_node(k_0)\n",
    "\n",
    "    k_0 = k_1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aG = DPA(N=1000, \n",
    "        fm=0.5, \n",
    "        d=0.01, \n",
    "        plo_M=2.5, \n",
    "        plo_m=2.5,\n",
    "        verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_degree_dist(aG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c356627e19890640c661bf35b313e3ac05771087fe3f4aaa57d60dc61ec63b4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
